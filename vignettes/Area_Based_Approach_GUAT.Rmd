---
title: "Area Characterisation in Guatemala"
date: " `r format(Sys.Date(),  '%d %B %Y')`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Area Characterisation in Guatemala}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Recent displacement trends show growing numbers of displaced population living outside of designated areas such as camp/camp like setting (traditional camps collective/transit/reception centres, informal settlements) with a majority setting in dispersed locations predominately urban and peri-urban areas such as informal settlements, unfinished buildings or interspersed in host community homes and communities, shared rooms or rental arrangements. To be able to reach, properly assess and understand local dynamics, vulnerabilities and capacities of the displaced and host populations alike, humanitarian organisations are increasingly using sub-national [Area Based Approach](https://www.humanitarianlibrary.org/collection/implementing-area-based-approaches). Area based approaches define “_an area, rather than a sector or target group, as a primary entry point_”. Such approach is particularly appropriate when residents in an affected area face complex, inter-related and multisectoral needs, resulting in risk of forced displacement.

# Area Based Approach for Forced Displacement: Characterisation through Sensor Data

In the context of migration statistics, forced displacement is often analyzed with the prism of [push and pull factors](https://immigrationforum.org/article/push-or-pull-factors-what-drives-central-american-migrants-to-the-u-s/). 

|                                                                                                | **Push Factor** (Mitigated by intervention to address root causes within countries of origin)         | **Pull Factor** (Mitigated by migration & Asylum policies of receiving countries)  |
|------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------|
| **Economic Dimension** (Addressed by programme in relation with development & poverty alleviation) | Lack of public services, Unemployement, Overpopulation                                                | More jobs, Better jobs, Higher wages,  promise of a “better individual life”       |
| **Social Dimension** (Addressed by programme in relation with protection)                          | Violence, insecurity, intolerance towards certain groups, active political or religious persecution,  | Safety, tolerance, freedom                                                         |
| **Environmental Dimension** (Addressed by programme in relation with resilience & sustainability) | Climate change, natural disasters                                                                     | More livable environment                                                           |

Though, traditional statistical data sources are often lacking sufficient geographically-fine-grained disaggregation to inform sub national scale approach and characterization.  Alternative based on sophisticated index like [Inform Colombia](https://drmkc.jrc.ec.europa.eu/inform-index/INFORM-Subnational-Risk/Colombia) requires extensive expert consultations and might not fully reflect the important dimension to be reflected in the context of forced displacement and migration. 

# Proposed methodology

New sensors provide unique abilities to capture new  flow of information from social medias (Anonymized data from Facebook platform) at subnational scale through grid level information. Satellite data can pick up signals of economic activity by detecting light at night, it can pick up development status by detecting infrastructure such as roads, and it can pick up signals for individual household wealth by detecting different building footprints and roof types. 

In regard to the framework above, an initial selection of globally available layers includes: 

 *  Economics:
       * Weighted Relative Wealth Index ([Facebook-RWI](https://data.humdata.org/dataset/relative-wealth-index))
       * Social Connectedness Index ([Facebook-SCI](https://data.humdata.org/dataset/social-connectedness-index))
       * Public Services Catchment area (OSM)
   
 *  Environment:
       * Agricultural drought frequency Index ([FAO-ASI](http://www.fao.org/giews/earthobservation/country/index.jsp?code=SLV&lang=en))
       * Climatic Natural Risk - Flood and Cyclone  (Prevention Web)
       * Geologic Natural Risk - Earthquake and Volcano  (Prevention Web)
  
 *  Social: 
       * Population Dependency Ratio (Facebook)
       * Movement Range data sets  [Facebook](https://data.humdata.org/dataset/movement-range-maps)
       * Violence (ACLED)
 
Information can be compiled and aggregated at admin level 2 in order to build composite Indicators. Different areas can be then grouped together based on the values from those composite indicators. The advantage of this approach are multiple:
 1. __Granularity__: Optimal Level of granularity
 2. __Availibility__: Data Consistently and freely available worldwide, simplicity to obtain information, ensor based indicators are potentially less sensitive to political pressure
 3. __Reproducibility__: Can be used in multiple countries easily and Fully automated and audited through reproducible analysis script

The resulting information can complement other traditional source of information both on quantitative (Household Survey) and qualitative (Focus Group Discussions) side.





```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = FALSE,
                      comment = "#>",
                      fig.align = "center")
knitr::opts_chunk$set(fig.width = 12, fig.height = 9, fig.retina = 2, fig.align = "center", dev.args = list(pointsize = 11))
set.seed(1)
extrafont::loadfonts(quiet=TRUE)
options(scipen = 999) # turn-off scientific notation like 1e+48
library(unhcRstyle)
library(magrittr)
library(readr)
library(sf)
library(purrr)
library(lubridate)
library(dplyr)
library(GADMTools)
library(tidyverse)
library(gghighlight)
library(dplyr)
library(zoo)
library(raster)
library(sf)
library(leaflet)
library(sp)
library(glue)
library(exactextractr)
library(furrr)
library(rmapshaper)
library(magick)
library(magrittr) 
library(mapsf)
library(potential) 
library(sp)
library(kableExtra)
library(matrixStats)

# remotes::install_gitlab("dickoa/rhxl") ## rhdx dependency
# remotes::install_gitlab("dickoa/rhdx") ## github mirror also avalailable
library(rhdx)

library(gganimate)
library(cowplot)
library(transformr)

mainDir <- getwd()
## If you save your analysis under vignette folder...
#mainDirroot <- substring(mainDir, 0 , nchar(mainDir) - 10)
mainDirroot <- mainDir

```



```{r getmap, echo = FALSE, message = FALSE, warning = FALSE, cache =TRUE}
# https://s3.us-east-1.amazonaws.com/hdx-production-filestore/resources/86f38f11-8062-4a09-b088-fbb581a051fd/gtm_adm_ocha_conred_2019_shp.zip 
## here we pull our geometry
polyLevel0 <- st_read( paste0(mainDirroot,"/data-raw/admin/GTM/gtm_admbnda_adm0_ocha_conred_20190207.shp"), quiet = TRUE)
polyLevel1 <- st_read( paste0(mainDirroot,"/data-raw/admin/GTM/gtm_admbnda_adm1_ocha_conred_20190207.shp"), quiet = TRUE)
polyLevel2 <- st_read( paste0(mainDirroot,"/data-raw/admin/GTM/gtm_admbnda_adm2_ocha_conred_20190207.shp"), quiet = TRUE)

# polyLevel0 <- raster::getData('GADM', country = 'SLV', level = 0)
# ## Convert the spatial file to sf to increase speed and ease of plotting
polyLevel0_sf <- st_as_sf(polyLevel0) %>%
  ms_simplify()
# 
# ## Check what you get
# ##plot(sf::st_geometry(polyLevel0_sf))
# 
# ## Fetch shapefiles 
# 
# polyLevel1 <- raster::getData('GADM', country = 'SLV', level = 1)
# ## Convert the spatial file to sf to increase speed and ease of plotting
polyLevel1_sf <- st_as_sf(polyLevel1) %>%
  ms_simplify()
# 
# 
# polyLevel2 <- raster::getData('GADM', country = 'SLV', level = 2)
# ## Convert the spatial file to sf to increase speed and ease of plotting
polyLevel2_sf <- st_as_sf(polyLevel2) %>%
  ms_simplify()
# #   st_simplify(preserveTopology=TRUE, dTolerance = .0025)

## Check what you get
##plot(sf::st_geometry(polyLevel2_sf))

ocean <- st_read( paste0(mainDirroot,"/data-raw/ocean/ne_50m_ocean.shp"), quiet = TRUE)


```



```{r getothermap, echo = FALSE, message = FALSE, warning = FALSE, cache =TRUE}
# Read in the detailed GADM shapes  - https://data.biogeo.ucdavis.edu/data/gadm3.6/gadm36_levels_shp.zip 
#shapes_in <- readRDS(paste0(mainDirroot,"/data-raw/gadm/gadm1_nuts3_counties.Rds"))
gadm1 <- st_read( paste0(mainDirroot,"/data-raw/gadm/gadm36_1.shp"), quiet = TRUE) #%>% 
  #ms_simplify()
##plot(sf::st_geometry(gadm1))

# USCounties <- raster::getData('GADM', country = 'USA', level = 2)
# levels(as.factor(USCounties$GID_2))
# Get the maps from the tigris package
library(tigris)
options(tigris_use_cache = TRUE)
counties_map <- tigris::counties(cb = TRUE) %>% 
  st_as_sf()  %>% 
  st_transform(crs=4326)  %>% 
  #st_transform(crs("+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"))  %>% 
  mutate(GID_1 = paste0("USA",STATEFP, COUNTYFP),
         NAME_1 = NAME)  %>% 
  ### Remov Hawai & Alaska, Guama , Samoa, Virgin islan..
  filter( !(STATEFP %in% c("15", "02", "60", "66", "69", "78"))) %>% 
  #select(GID_1, NAME_1) %>%
  ms_simplify()

counties_map <- counties_map[ , c("GID_1", "NAME_1", "geometry")]

#levels(as.factor(counties_map$STATEFP))

#plot(sf::st_geometry(counties_map))
```



```{r mapsftheme, echo = FALSE, message = FALSE, warning = FALSE, cache =TRUE}
##Set up the theme for all maps below! 

## Sea #B3D8F0
## Name #828993
## Border #93A3AB
## Country #E2E7EB
## Feature country #FFFFFF
##

# Select a font already installed on your system !!
par(family="Lato")
# set a theme
mapsf::mf_theme(bg = "#E2E7EB",  ## background color --> Used country 
                # bg = "#cdd2d4", "#faebd7ff",  "#cdd2d4",
                mar = c(0, 0, 2, 0), ## margins
                tab = FALSE,  # if TRUE the title is displayed as a 'tab'
                fg = "#0072BC",  ## foreground color --> for the top title - use UNHCR Blue..
                pos = "left", # position, one of 'left', 'center', 'right'
                inner = FALSE, # if TRUE the title is displayed inside the plot area.
                line = 2, #number of lines used for the title
                cex = 1.5, #cex of the title
                #font = "Lato",
                font = 1 ) #font of the title



```

# Social: 

## Population Dependency Ratio (Facebook)

```{r getdatapop, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}

# https://data.humdata.org/dataset/guatemala-high-resolution-population-density-maps-demographic-estimates

population_all <- rhdx::pull_dataset("guatemala-high-resolution-population-density-maps-demographic-estimates") %>%
            get_resource(2) %>%
            read_resource() %>%
            as("Raster")
polyLevel2_sf$total_pop <- exactextractr::exact_extract(population_all, polyLevel2_sf,
                                      fun = 'sum',
                                      progress = FALSE)

population_under5 <- pull_dataset("guatemala-high-resolution-population-density-maps-demographic-estimates") %>%
  get_resource(4) %>%
  read_resource() %>%
  as("Raster")

polyLevel2_sf$under5 <- exactextractr::exact_extract(population_under5, polyLevel2_sf,
                                        fun = 'sum',
                                        progress = FALSE)

population_60 <- pull_dataset("guatemala-high-resolution-population-density-maps-demographic-estimates") %>%
  get_resource(6) %>%
  read_resource() %>%
  as("Raster")
polyLevel2_sf$elderly_pop <- exactextractr::exact_extract(population_60, polyLevel2_sf,
                                        fun = 'sum',
                                        progress = FALSE)

reproductive_women <- pull_dataset("guatemala-high-resolution-population-density-maps-demographic-estimates") %>%
  get_resource(12) %>%
  read_resource() %>%
  as("Raster")
polyLevel2_sf$reproductive_women <- exactextractr::exact_extract(reproductive_women, polyLevel2_sf,
                                        fun = 'sum',
                                        progress = FALSE)

youth <- pull_dataset("guatemala-high-resolution-population-density-maps-demographic-estimates") %>%
  get_resource(14) %>%
  read_resource() %>%
  as("Raster")
polyLevel2_sf$youth <- exactextractr::exact_extract(youth, polyLevel2_sf,
                                        fun = 'sum',
                                        progress = FALSE)



polyLevel2_sf <- polyLevel2_sf %>%
### % of elderly
  mutate(percent_elderly = elderly_pop / total_pop)%>%
### % of elderley
  mutate(percent_under5 = under5 / total_pop)%>%
### % of reproductive_women
  mutate(percent_reproductive_women = reproductive_women / total_pop)%>%
### % of youth
  mutate(percent_youth = youth / total_pop)


rm(youth, reproductive_women, population_60, population_under5,
   population_all)
```


```{r echo = FALSE, message = FALSE, warning = FALSE}
 # https://data.humdata.org/dataset/relative-wealth-index 
population <-    read.csv(paste0(mainDirroot,"/data-raw/population/GTM/gtm_general_2020.csv")) 

populationsp <- st_as_sf(population, coords = c("longitude", "latitude"), crs = 4326)

#world <- st_as_sf(world.spdf)
# Discretize the variable
bv <- quantile(population$population_2020, seq(from = 0, to = 1, length.out = 9))
# Draw the map
# Set a color palette
pal <- mf_get_pal(n = 9, palette = "RdPu", rev = TRUE)
```


```{r echo = FALSE, message = FALSE, warning = FALSE}

mapsf::mf_init(populationsp)
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
 
# Map the regional GDP per capita
mapsf::mf_map(x = populationsp, 
              var = "population_2020", 
              type = "choro",
       leg_pos = "topright",
       breaks = bv, 
       pal = pal, 
       border = NA, 
       leg_title = "Population Density",
     #  leg_val_rnd = -2, 
       add = TRUE)
# Set a layout
mapsf::mf_title(txt = "Population Density in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: Facebook Data For Good", 
           bg = "#ffffff80")


```

```{r}

# 
# p <- ggplot(st_transform(populationsp, "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")) +
#       geom_sf(aes(fill = population_2020), 
#               colour="#DADADA", 
#               size=0.25) +
#       #scale_fill_brewer(palette = "RdPu", na.value="#F5F5F5", drop=FALSE) +
#   
#       labs(title = "Population Density in El Salvador" ,
#            subtitle = "As of August 2018",
#            x = "",
#            y = "",
#            fill = "SCI", 
#            caption = "Source: Facebook Data For Good ") + 
#       
#       unhcRstyle::unhcr_theme(base_size = 12)  + ## Insert UNHCR Style
#       theme(#panel.grid.major = element_line(color = gray(.5),  linetype = "dashed", size = 0.5),
#             panel.grid.major = element_blank(),
#             panel.background = element_rect(fill = "aliceblue"),
#             axis.text.x  = element_blank(),
#             axis.text.y  = element_blank(),
#             legend.title = element_blank(), 
#             legend.text  = element_text(size = 8),
#             legend.key.size = unit(0.8, "lines"),
#             legend.position = "bottom", 
#             legend.box = "horizontal"
#             ) 
# p
```



```{r echo = FALSE, message = FALSE, warning = FALSE}



# # # Compute Stewart potentials
# populationstewart <- SpatialPosition::stewart(knownpts = populationsp,
#                                              varname = "population_2020",
#                                              mask = polyLevel0_sf,
#                                              typefct = "exponential",
#                                             ## numeric; distance where the density of probability of the spatial interaction function equals 0.5.
#                                              span = 200,
#                                             ## numeric; impedance factor for the spatial interaction function.
#                                              beta = 1,
#                                              returnclass = "sf")
# 
# # Create contour
# populationcontour <- SpatialPosition::isopoly(x = populationstewart,
#                                                    nclass = 6,
#                                                    mask = polyLevel0_sf, 
#                                                    returnclass = "sf")

```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# populationspgrid <- potential::create_grid(x = populationsp, 
#                                           res = 200000)
# 
# plot(sf::st_geometry(populationsp))
# plot(sf::st_geometry(populationspgrid))
# 
# populationmatrix <- potential::create_matrix(populationsp, populationspgrid)
# 
# populationsp$stewart<- potential::potential(
#                                       x = populationsp, 
#                                       y = populationspgrid, 
#                                       d = populationmatrix, 
#                                       var = "population_2020",
#                                       fun = "e", 
#                                       span = 200000, 
#                                       beta = 2 
#                                     )
# 
# 
# populationequipot <- potential::equipotential(populationsp, 
#                                     var = "stewart", 
#                                     mask = polyLevel0_sf)
# 
# plot(populationequipot["center"], 
#      pal = hcl.colors(nrow(equipot),"cividis") )
# 
# 
# 
# # Created breaks
# bks <- sort(unique(c(populationcontour$min, populationcontour$max)))

```

```{r echo = FALSE, message = FALSE, warning = FALSE}  
# Draw the basemap

# 
# mapsf::mf_init(populationcontour)
# # mapsf::mf_map(polyLevel0_sf, 
# #               col = "#f5f5f3ff", 
# #               border = "#a9b3b4ff", 
# #               add = TRUE)
# # Map the regional GDP per capita
# mapsf::mf_map(x = populationcontour, 
#               var = "center", 
#               type = "choro",
#               leg_pos = "topright",
#               breaks = bks, 
#               pal = pal, 
#               border = NA, 
#               leg_title = "Population Density",
#               #leg_val_rnd = -2, 
#               add = TRUE)
# # Set a layout
# mapsf::mf_title(txt = "Population Density in El Salvador", 
#                fg = "#FFFFFF")
# mapsf::mf_credits(txt = "Source: Facebook Data For Good", 
#                    bg = "#ffffff80")

#mapsf::mf_scale()
# Set a text to explicit the function parameters
# text(x = 6271272, y = 3743765, 
#      labels = "Distance function:\n- type = exponential\n- beta = 2\n- span = 75 km", 
#      cex = 0.8, adj = 0, font = 3)
 
```




## Population movement range 

The  [Movement Range data sets ](https://data.humdata.org/dataset/movement-range-maps) is intended to inform on how populations are [responding to physical distancing measures](https://research.fb.com/blog/2020/06/protecting-privacy-in-facebook-mobility-data-during-the-covid-19-response/). In particular, there are two metrics that provide a slightly different perspective on movement trends:

 * __Change in Movement__: looks at how much people are moving around and compares it with a baseline period that predates most social distancing measures. The idea is to understand how much less people are moving around since the onset of the coronavirus epidemic. This is done by quantifying how much people move around by counting the number of level-16 Bing tiles (which are approximately 600 meters by 600 meters in area at the equator) they are seen in within a day. In the dataset noted `all_day_bing_tiles_visited_relative_change`  
 
 * __Stay Put__: looks at the fraction of the population that appear to stay within a small area during an entire day. This metric intends to measure this by calculating the percentage of eligible people who are only observed in a single level-16 Bing tile during the course of a day. In the dataset noted `all_day_ratio_single_tile_users`



```{r getdatamovement, echo = FALSE, message = FALSE, warning = FALSE}

# zip_file_path <- pull_dataset("movement-range-maps") %>%
#   get_resource(2) %>%
#   download_resource()
# 
# files <- unzip(zip_file_path, list = TRUE)
# files <- grep("movement-range", files$Name, value = TRUE)
# 
# file_path <- unzip(zip_file_path, files = files)

file_path <-    paste0(mainDirroot,"/data-raw/movement/movement-range-2021-10-02.txt") 

movement_range <- read_delim(file_path, delim = "\t") %>%
  mutate(
    GID_0 = country,
   # GID_2 = polygon_id,
    #NAME_1 = gadm1_name,
    NAME_2 = polygon_name
  ) %>% 
  filter(ds >= as.Date("2020-07-25"),
         ds <= as.Date("2021-10-02")) 


md <- subset(movement_range,country %in% c( "SLV" #, "HND", "MEX","BLZ", "GTM"
                                            ))


md$ds <- as.Date(md$ds, "%Y-%m-%d") 
#table(md$polygon_name)

#as.character(unique(md$polygon_name))

## Find the maximum and minimum observed stay at home values for all regions and all dates
max_stay_put = max(md$all_day_ratio_single_tile_users)
min_stay_put = min(md$all_day_ratio_single_tile_users)
#glimpse(mli_mov)
 
# levels(as.factor(df$NAME_2))
# levels(as.factor(df$GID_2))


# polyLevel2_sf <- polyLevel2_sf %>%
# ### Calculate weighted risk score
#   mutate(risk_score = (percent_elderly * 100) * 
#            (100 - (all_day_ratio_single_tile_users * 100)) * elderly_pop) %>%
# ### Calculate gaduate risk score
#   mutate(risk_score_scaled = 100 * (risk_score - min(risk_score, na.rm = TRUE)) / diff(range(risk_score, na.rm = TRUE)))


# ggplot(polyLevel2_sf) +
#   geom_sf(aes(fill = risk_score_scaled)) +
#   scale_fill_viridis_c(direction = -1) +
#   theme_map()
```


```{r getdatamovement1, echo = FALSE, message = FALSE, warning = FALSE}
# movement_range <- rbind(utils::read.delim(paste0(mainDirroot,"/data-raw/movement/movement-range-2021-09-18.txt")),
#                         utils::read.delim(paste0(mainDirroot,"/data-raw/movement/movement-range-data-2020-03-01--2020-12-31.txt")) )

#table(movement_range$country)

# This data includes movement changes measured by Facebook starting from a baseline in February. 
# 
# 
#   * `ds`: Date stamp for movement range data row in YYYY-MM-DD form
#   * `country`: Three-character ISO-3166 country code
#   * `polygon_source`: Source of region polygon, either  FIPS  for U.S. data or  GADM  for global data
#   * `polygon_id`: Unique identifier for region polygon, either numeric string for U.S. FIPS codes or alphanumeric string for GADM regions
#   * `polygon_name`: Region name
#   * `all_day_bing_tiles_visited_relative_change`: Positive or negative change in movement relative to baseline
#   * `all_day_ratio_single_tile_users`: Positive proportion of users staying put within a single location
#   * `baseline_name`: When baseline movement was calculated pre-COVID-19
#   * `baseline_type`: How baseline movement was calculated pre-COVID-19


```

```{r plot2, echo = FALSE, message = FALSE, warning = FALSE}
## I strongly recommend to subset unless you want the entire globe. This will be very large and slow if you do not.
md <- subset(movement_range,country %in% c( "SLV" #, "HND", "MEX","BLZ", "GTM"
                                            ))


md$ds <- as.Date(md$ds, "%Y-%m-%d") 
#table(md$polygon_name)

#as.character(unique(md$polygon_name))

## Find the maximum and minimum observed stay at home values for all regions and all dates
max_stay_put = max(md$all_day_ratio_single_tile_users)
min_stay_put = min(md$all_day_ratio_single_tile_users)


## These filtered versions of the data determine the polygon_id which define the overall maximum and minimum values 
md_filtered <- md %>%
  group_by(polygon_id) %>% 
  filter(max(all_day_ratio_single_tile_users) >= max_stay_put) %>%
  ungroup()

md_filtered2 <- md %>%
  group_by(polygon_id) %>% 
  filter(min(all_day_ratio_single_tile_users) <= min_stay_put) %>%
  ungroup()

```
 
```{r pressure1, echo = FALSE, message = FALSE, warning = FALSE}
## Produce a histogram of all stay put values for the country. Use this to judge how many standard deviations should be used to define best and worst regions
ggplot(md, aes(x=all_day_ratio_single_tile_users)) + 
  geom_histogram(color="black", fill="white",bins = 50) +
  scale_y_continuous( label = format_si()) + ## Format axis number
  geom_vline(aes(xintercept=median(all_day_ratio_single_tile_users)),
             color="blue", 
             linetype="dashed", 
             size=1) +
  labs(x = "Ratio of users staying all day within a single cartographic tile", 
                 y = " ", 
                 title = "How mobile people are?",
                 subtitle = "In Average, 20% of FB users stayed wtihin the same closed location",
                 caption = "Source: Facebook users Staying Put - Facebook Data For Good - one tile is 600 meters by 600 meters") +
            geom_hline(yintercept = 0, size = 1.1, colour = "#333333") +
            unhcRstyle::unhcr_theme(base_size = 8)   + ## Insert UNHCR Style
            theme(panel.grid.major.y  = element_line(color = "#cbcbcb"), 
                  panel.grid.major.x  = element_blank(), 
                  panel.grid.minor = element_blank()) 

```


```{r plot3, echo=FALSE}

# 
# # stay put values for regions within one sd of best performance recorded overlaid above time series of all region
# 
# ## Line graphs of stay put values for regions within one sd of best performance recorded overlaid above time series of all regions
# ggplot(md) +
#   geom_line(aes(ds, all_day_ratio_single_tile_users, colour = polygon_name)) +
#   gghighlight(max(all_day_ratio_single_tile_users) >= max_stay_put-sd(md_filtered$all_day_ratio_single_tile_users),
#               use_direct_label = FALSE,
#               unhighlighted_params = list(colour = alpha("grey", 0.1))) + 
#   
#   facet_wrap(~ polygon_name) +
#   
#   scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
#   geom_vline(aes(xintercept=median(all_day_ratio_single_tile_users)),
#              color="blue", 
#              linetype="dashed", 
#              size=1) +
#   labs(x = "Ratio of users staying all day within a single cartographic tile", 
#                  y = " ", 
#                  title = "How mobile people are?",
#                  subtitle = "In Average, 20% of FB users stayed wtihin the same closed location",
#                  caption = "Source: Facebook users Staying Put - Facebook Data For Good - one tile is 600 meters by 600 meters") +
#             geom_hline(yintercept = 0, size = 1.1, colour = "#333333") +
#             unhcRstyle::unhcr_theme(base_size = 8)   + ## Insert UNHCR Style
#             theme(legend.position = "none",
#                   panel.grid.major.y  = element_line(color = "#cbcbcb"), 
#                   panel.grid.major.x  = element_blank(), 
#                   panel.grid.minor = element_blank(),
#                   axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
#   
    


```

```{r plot4, echo = FALSE, message = FALSE, warning = FALSE}
## Line graphs of stay put values for regions within one sd of worst performance 
# recorded overlaid above time series of all regions
ggplot(data = md) +
   geom_line(aes(x = ds, 
                 y = all_day_ratio_single_tile_users, 
                 #colour = polygon_name,
                 group = polygon_name)) +
   gghighlight(min(all_day_ratio_single_tile_users) <= min_stay_put + sd(md_filtered2$all_day_ratio_single_tile_users),
               use_direct_label = FALSE, 
               unhighlighted_params = list(colour = alpha("grey", 0.1))) + 
  
   facet_wrap(~ polygon_name) +
   
  scale_x_date(labels = scales::date_format("%b")) +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  geom_vline(aes(xintercept=median(all_day_ratio_single_tile_users)),
             color="blue", 
             linetype="dashed", 
             size=1) +
  labs(x = "Ratio of users staying all day within a single cartographic tile", 
       y = " ", 
       title = "How mobile people are?",
       subtitle = "Highlighted area the one beyond Standard Deviation ",
       caption = "Source: Facebook users Staying Put - Facebook Data For Good - one tile is 600 meters by 600 meters") +
  geom_hline(yintercept = 0, size = 1.1, colour = "#333333") +
  unhcRstyle::unhcr_theme(base_size = 8)   + ## Insert UNHCR Style
  theme(legend.position = "none",
                  panel.grid.major.y  = element_line(color = "#cbcbcb"), 
                  panel.grid.major.x  = element_blank(), 
                  panel.grid.minor = element_blank(),
                  axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 
  
  
 
```

```{r echo = FALSE, message = FALSE, warning = FALSE}
# ## Some fields require renaming to match the field names in the sf shapefile. Each country's polyLevel2.sf will be slightly different in how it encodes the admin fields, so it's not plug and play.
# 
# # 
# # 
# # levels(as.factor(polyLevel2_sf$GID_2))
# 
# ## Setup the date ranges and trim the metric outliers
# ranges = df %>%
#   filter(ds >= '2020-03-01',
#          !is.na(all_day_ratio_single_tile_users)) %>%
#   summarise(p05 = quantile(all_day_ratio_single_tile_users,0.01),
#             p95 = quantile(all_day_ratio_single_tile_users,0.99))
# 
# ## Will try and plot this date first, but then go to the next earliest if not available in the df dataframe
# target_ds = max(df$ds)-180
# df <- subset(df, ds >= target_ds)
# 
# ## FUN to generate a map plot for each day of data and save to local disk
# plot_date <- function(joined_spatial, target_ds) {
#   data = joined_spatial %>%
#     filter(ds == target_ds) %>%
#     inner_join(polyLevel2_sf) %>%
#     st_sf()
#   
#   minx = plyr::round_any(ranges$p05, 0.05, ceiling)
#   maxx = plyr::round_any(ranges$p95, 0.05, ceiling)
#   #probably where you set the start date of the dots animation scale
#   days = as.double(difftime(ymd(target_ds),
#                             ymd('2021-01-01'),
#                             units = "days")) + 1
#   print(target_ds)
#   ##dots = paste0(rep('.', days), collapse = '')
#   title = glue('\n Date: {target_ds}')   # \n{dots}')
# 
#   
# datg <- data %>%
#     mutate(all_day_ratio_single_tile_users = case_when(all_day_ratio_single_tile_users < minx ~ minx,
#                                                          all_day_ratio_single_tile_users > maxx ~ maxx,
#                                                          TRUE ~ all_day_ratio_single_tile_users))  
#     
# g <-  ggplot() + 
#     
#     geom_sf(data = datg,
#             aes(fill = plyr::round_any(all_day_ratio_single_tile_users, 0.05)),
#             size = 0.02, 
#             color = '#ccbbbb') +
#     
#     # geom_sf(polyLevel2_sf,
#     #           colour="black",
#     #           size=0.2) +
#     # palette = 'YlGnBu' for All, palette = 'PuRd' and direction 1 for Restaurants
#     scale_fill_distiller(type = 'seq', palette = 'YlGnBu', direction = 1,
#                          limits = c(0, 1.0), # use maxx instead of 1.0 if you want to scale the value to the underlying data
#                          labels = scales::percent_format(accuracy = 5)) +
#   
#       
#       unhcRstyle::unhcr_theme(base_size = 12)  + ## Insert UNHCR Style
#       theme(#panel.grid.major = element_line(color = gray(.5),  linetype = "dashed", size = 0.5),
#             panel.grid.major = element_blank(),
#             panel.background = element_rect(fill = "aliceblue"),
#             axis.text.x  = element_blank(),
#             axis.text.y  = element_blank(),
#             legend.title = element_blank(), 
#             legend.text  = element_text(size = 8),
#             legend.key.size = unit(0.8, "lines"),
#             legend.position = "bottom", 
#             legend.box = "horizontal"
#             )
#   
#   # unhcRstyle::unhcr_theme(base_size = 8)   + ## Insert UNHCR Style
#   # theme(  panel.grid = element_blank(),
#   #         legend.key.height = unit(0.25, 'cm'),
#   #         legend.key.width = unit(0.10, 'cm'),
#   #         axis.text = element_blank(),
#   #         plot.margin = unit(c(0.1,0.1,0.1,0.1), "cm"),
#   #         panel.spacing=unit(c(0,0,0,0), "null"),
#   #         legend.position = c(0.1, 0.1)) +
#   labs(fill = 'Facebook Data for Good \n% Staying Put',
#          title = title)
#   ggsave(glue({'anim/SLV-{target_ds}-staying put.png'}), g, scale = 1.0,
#          width = unit(4, 'cm'),
#          height = unit(4, 'cm'))
#   g
# }
# 
# ## Create a subdirectory so that you don't flood your working directory with many png files
# if (!fs::dir_exists("anim")) fs::dir_create("anim")
# 
# 
# ## Indicate the start of the date range. Ideally the min ds in df, but also allows hand manipulation if you want to use the dots visualization in the plots as a cheap timeline
# date_range = df %>%
#   filter(ds >= '2020-03-01') %>%
#   distinct(ds) %>%
#   pull(ds)
# 
# ## Might not be needed, but can help with not grinding a local machine while generating the plots
# plan(multisession)
# 
# 
# ## FUN CALL
# plot_date(df, target_ds)

# ## kicks off iterating through all dates.
# results = future_map(date_range, ~plot_date(df, .x))



# list.files(path='anim/', pattern = '*.png', full.names = TRUE) %>% 
#   image_read() %>% # reads each path file
#   image_join() %>% # joins image
#   image_morph(frames = 2) %>%
#   image_animate(fps=20) %>% # animates, can opt for number of loops
#   magick::image_write("anim/FileName.gif") # write to current dir

```


## Violence

```{r getdataacled, message=FALSE, warning=FALSE,  echo = FALSE,  include=FALSE, cache = TRUE}
## getting data from ACLED for events... 
acled.data <- acled.api::acled.api(email.address = Sys.getenv("EMAIL_ADDRESS"),  # see https://developer.acleddata.com/
                                   access.key = Sys.getenv("ACCESS_KEY"),
                                   start.date = "2017-01-01",
                        end.date = lubridate::today(),
                        country = c( "El Salvador"),
                        all.variables = TRUE) 



acledsp <- st_as_sf(acled.data, coords = c("longitude", "latitude"), crs = 4326)  %>%
 # find points within polygons
 st_join(., polyLevel2_sf, join = st_within) 


# levels(as.factor(acled.data$event_type))
# levels(as.factor(acled.data$sub_event_type ))
event <- as.data.frame(unique(acled.data[ ,c("event_type","sub_event_type")]))

#View(acled.data[ acled.data$sub_event_type == "Sexual violence", ])

polyLevel2_sf <- polyLevel2_sf %>%
  left_join(
## Compile some stat by admin 2
#acled.level2 <-
  acledsp %>%
  st_drop_geometry() %>%
  group_by(ADM2_PCODE) %>%
 # summarise()%>% 
  #count( sub_event_type) %>% 
  dplyr::count( event_type) %>% 
  group_by(ADM2_PCODE) %>%
  #pivot_wider(names_from = sub_event_type, values_from = n) %>%
  pivot_wider(names_from = event_type, values_from = n) %>%
  janitor::clean_names() %>%
  mutate(ADM2_PCODE = adm2_pcode  ) ,
 by = "ADM2_PCODE")%>%
  left_join(
## Compile some stat by admin 2
#acled.level2 <-
  acledsp %>%
  st_drop_geometry() %>%
  group_by(ADM2_PCODE) %>%
 # summarise()%>% 
   dplyr::count( sub_event_type) %>% 
 # count( event_type) %>% 
  group_by(ADM2_PCODE) %>%
  pivot_wider(names_from = sub_event_type, values_from = n) %>%
  #pivot_wider(names_from = event_type, values_from = n) %>%
  janitor::clean_names() %>%
  mutate(ADM2_PCODE = adm2_pcode  ) ,
 by = "ADM2_PCODE")

```


```{r}
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "battles",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)

# Set a layout
mapsf::mf_title(txt = "Battles in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```


```{r}
#names(polyLevel2_sf)
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "explosions_remote_violence",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)
 
# Set a layout
mapsf::mf_title(txt = "Explosions & Remote Violence in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```
```{r}
#names(polyLevel2_sf)
# "protests"                           "strategic_developments"            
#"violence_against_civilians"         "riots"   
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "protests",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)
 
# Set a layout
mapsf::mf_title(txt = "Protests in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```

```{r}
#names(polyLevel2_sf)
# "protests"                           "strategic_developments"            
#"violence_against_civilians"         "riots"   
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "strategic_developments",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)
 
# Set a layout
mapsf::mf_title(txt = "Strategic developmentsin El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```

```{r}
#names(polyLevel2_sf)
# "protests"                           "strategic_developments"            
#"violence_against_civilians"         "riots"   
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "violence_against_civilians",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)
 
# Set a layout
mapsf::mf_title(txt = "Violence against Civilians in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```

```{r}
#names(polyLevel2_sf)
# "protests"                           "strategic_developments"            
#"violence_against_civilians"         "riots"   
mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map( 
  x = polyLevel2_sf, 
  var = "riots",
  type = "prop",
  inches = 0.25, 
  col = "brown4",
  leg_pos = "bottomleft2",  
  leg_title = ""
)
 
# Set a layout
mapsf::mf_title(txt = "Riots in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: ACLED - aggregated data per level 2 since 2018",
                   bg = "#ffffff80")
```



```{r echo = FALSE, message = FALSE, warning = FALSE} 
# acledstewart <- SpatialPosition::stewart(knownpts = acledsp, 
#                      varname = "fatalities",
#                      mask = polyLevel0_sf,
#                      typefct = "exponential", 
#                     ## numeric; distance where the density of probability of the spatial interaction function equals 0.5.
#                      span = 6000,  
#                     ## numeric; impedance factor for the spatial interaction function.
#                      beta = 5, 
#                      returnclass = "sf")
# # Create contour
# acledcontour <- SpatialPosition::isopoly(x = acledstewart,
#                        nclass = 6,
#                        mask = polyLevel0_sf, 
#                        returnclass = "sf")
# 
# # Created breaks
# bks <- sort(unique(c(acledcontour$min, acledcontour$max)))
# 
# 
# # Set a color palette
# pal <- mf_get_pal(n = 9, palette = "Burg", rev = TRUE)  
# 
# mapsf::mf_init(acledcontour)
# mapsf::mf_map(ocean, 
#               col = "#B3D8F0", 
#               border = NA, 
#               add = TRUE)
# mapsf::mf_map(polyLevel2_sf, 
#               add = TRUE, 
#               lwd = 0.5, 
#               border = "#93A3AB", 
#               col = "#FFFFFF")
# # mapsf::mf_map(polyLevel0_sf, 
# #               col = "#f5f5f3ff", 
# #               border = "#a9b3b4ff", 
# #               add = TRUE)
# # Map the regional GDP per capita
# mapsf::mf_map(x = acledcontour, 
#               var = "center", 
#               type = "choro",
#               leg_pos = "topright",
#               breaks = bks, 
#               pal = pal, 
#               border = NA, 
#               leg_title = "Number of fatalities",
#               #leg_val_rnd = -2, 
#               add = TRUE)

# # Set a layout
# mapsf::mf_title(txt = "Fatalities in El Salvador", 
#                fg = "#FFFFFF")
# mapsf::mf_credits(txt = "Source: ACLED", 
#                    bg = "#ffffff80")


```




# Economics

## Relative Wealth Index

Many critical policy decisions, from strategic investments to the allocation of humanitarian aid, rely on data about the geographic distribution of wealth and poverty. 

As explained in a dedicated [paper](https://www.researchgate.net/publication/341999364_The_Relative_Value_of_Facebook_Advertising_Data_for_Poverty_Mapping/download), the [Relative Wealth Index](https://data.humdata.org/dataset/relative-wealth-index) estimates are built by applying machine learning algorithms to vast and heterogeneous data from satellites, mobile phone networks, topographic maps, as well as aggregated and de-identified connectivity data from Facebook.


As described in the this [tutorial](https://dataforgood.facebook.com/dfg/docs/tutorial-calculating-population-weighted-relative-wealth-index) 

  * Determine which administrative unit contains the centroid of each RWI tile  
  
  * Calculate the bing tile quadkey at zoom level 14 for each point in the population density dataset and sum the population per level 14 tile  
  
  * Determine which zoom level 14 (~2.4km bing tile) corresponds to each of the smaller 30m population density tiles, and calculate the sum of population within each zoom level 14 tile.  
  
  * Calculate the total population in each administrative region using the population density dataset  
  
  * Calculate a population derived weight for each zoom level 14 RWI tile  
  
  * Use the weight value to calculate a weighted RWI value and aggregate to the administrative unit level
  


```{r getdatawealth, echo = FALSE, message = FALSE, warning = FALSE}

# zip_file_path <- pull_dataset("relative-wealth-index") %>%
#   get_resource(1) %>%
#   download_resource()
# 
# files <- unzip(zip_file_path, list = TRUE)
# files <- grep("SLV_relative_wealth_index", files$Name, value = TRUE)
# 
# file_path <- unzip(zip_file_path, files = files[1])
file_path <-  paste0(mainDirroot,"/data-raw/wealth/GTM/gtm_relative_wealth_index.csv") 
wealth <- read.csv(file_path)



# wealth <-   read.csv(paste0(mainDirroot,"/data-raw/wealth/SLV_relative_wealth_index.csv")) 

# wealth <-  rbind(read.csv(paste0(mainDirroot,"/data-raw/wealth/SLV_relative_wealth_index.csv")) ,
#                   read.csv(paste0(mainDirroot,"/data-raw/wealth/HND_relative_wealth_index.csv")),
#                   read.csv(paste0(mainDirroot,"/data-raw/wealth/GTM_relative_wealth_index.csv")),
#                   read.csv(paste0(mainDirroot,"/data-raw/wealth/MEX_relative_wealth_index.csv"))
#                  )

wealthsp <- st_as_sf(wealth, coords = c("longitude", "latitude"), crs = 4326)

##plot(sf::st_geometry(wealthsp))
polyLevel2_sf <- polyLevel2_sf %>%
  left_join(
# find points within polygons
wealth_in_level2 <-
       st_join(wealthsp, polyLevel2_sf, join = st_within) %>%
       st_drop_geometry() %>%
                    group_by(ADM2_PCODE) %>%
                    summarise(rwi_avg =  mean(rwi ),
                              rwi_sd =  sd(rwi ),
                              rwi_max =  max(rwi ),
                              rwi_min =  min(rwi ))  ,
 by = "ADM2_PCODE")

# Quadkey reference JavaScript implementations
# https://developer.here.com/documentation/traffic/dev_guide/common/map_tile/topics/quadkeys.html
# 
# tileXYToQuadKey = function(xTile, yTile, z) {
#   quadKey = ""
#   for (i in z:1) {
#     digit = 0
#     mask = bitwShiftL(1, i - 1)
#     xtest = as_binary(bitwAnd(xTile, mask))
#     if(any(xtest)) {
#       digit = digit + 1
#     }
# 
#     ytest = as_binary(bitwAnd(yTile, mask))
#     if(any(ytest)) {
#       digit = digit + 2
#     }
#     quadKey = paste0(quadKey, digit)
#   }
#   quadKey
# }
# 


# wealthgrid <- CreateGrid(w = wealthsp, resolution = 3000, returnclass = "sf")
# # Create a distance matrix between known points (hospital) and mygrid
# mymat <- CreateDistMatrix(knownpts = wealthsp, unknownpts = wealthgrid)
# # Compute  distance weighted mean from known points (hospital) on a given
# # grid (mygrid) using a given distance matrix (mymat)
# wealthsmoothy <- smoothy(knownpts = wealthsp, 
#                      unknownpts = wealthgrid,
#                      matdist = mymat, 
#                      varname = "rwi",
#                      typefct = "exponential", 
#                      span = 1250,
#                      beta = 3, 
#                      mask = polyLevel0_sf, 
#                      returnclass = "sf")





```


```{r, echo = FALSE, message = FALSE, warning = FALSE}
# Discretize the variable
bv <- quantile(wealth$rwi, seq(from = 0, to = 1, length.out = 5))
# Draw the map
# Set a color palette
pal <- mf_get_pal(n = 5, 
                  #palette = "Burg", 
                  pal = c("Reds 2", "Greens"),
                rev = TRUE)

 

mapsf::mf_init(wealthsp)
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
# Map the regional GDP per capita
mapsf::mf_map(x = wealthsp, 
              var = "rwi", 
              type = "choro",
       leg_pos = "topright",
       breaks = bv, 
       pal = pal, 
       border = NA, 
       leg_title = "",
       #leg_val_rnd = -2, 
       add = TRUE)

# Set a layout
mapsf::mf_title(txt = "Relative Wealth Index in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: Facebook Data For Good", 
           bg = "#ffffff80")


```


```{r echo = FALSE, message = FALSE, warning = FALSE}
# Compute Stewart potentials
wealthstewart <- SpatialPosition::stewart(knownpts = wealthsp, 
                     varname = "rwi",
                     mask = polyLevel0_sf,
                     typefct = "exponential", 
                    ## numeric; distance where the density of probability of the spatial interaction function equals 0.5.
                     span = 1000,  
                    ## numeric; impedance factor for the spatial interaction function.
                     beta = 1, 
                     returnclass = "sf")
# Create contour
wealthcontour <- SpatialPosition::isopoly(x = wealthstewart,
                       nclass = 6,
                       mask = polyLevel0_sf, 
                       returnclass = "sf")

# Created breaks
bks <- sort(unique(c(wealthcontour$min, wealthcontour$max)))


# Set a color palette
pal <- mf_get_pal(n = 9, palette = "Burg", rev = TRUE)  
```

```{r echo = FALSE, message = FALSE, warning = FALSE}  
 

mapsf::mf_init(wealthcontour)
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map(x = wealthcontour, 
              var = "center", 
              type = "choro",
              leg_pos = "topright",
              breaks = bks, 
              pal = pal, 
              border = NA, 
              leg_title = "Relative Wealth Index",
              #leg_val_rnd = -2, 
              add = TRUE) 
# Set a layout
mapsf::mf_title(txt = "Relative Wealth Index in El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: Facebook Data For Good", 
                   bg = "#ffffff80")

#mapsf::mf_scale()
# Set a text to explicit the function parameters
# text(x = 6271272, y = 3743765, 
#      labels = "Distance function:\n- type = exponential\n- beta = 2\n- span = 75 km", 
#      cex = 0.8, adj = 0, font = 3)

  
  # 
  # opar <- par(mar = c(0,0,1.2,0))
  # # Display the map
  # choroLayer(x = contourpoly,
  #            var = "center", 
  #            legend.pos = "topleft",
  #            breaks = bks, 
  #            border = "grey90",
  #            lwd = 0.2,
  #            legend.values.rnd = 2,
  #            legend.title.txt = "Class")
  # plot(st_geometry(paris), add = TRUE)
  # layoutLayer(title = "Relative Wealth Index",
  #             sources = "", author = "")
  # par(opar)
```


##   Social Connectedness Index


an anonymized snapshot of all active Facebook users and their friendship networks to measure the intensity of connectedness between locations. The [Social Connectedness Index (SCI)](https://www.aeaweb.org/articles?id=10.1257/jep.32.3.259) is a measure of the social connectedness between different geographies. Specifically, it measures the relative probability that two individuals across two locations are friends with each other on Facebook.


http://pages.stern.nyu.edu/~jstroebe/PDF/BGHKRS_InternationalTradeSocialConnectedness_JIE.pdf 

https://data.humdata.org/dataset/social-connectedness-index 

```{r echo = FALSE, message = FALSE, warning = FALSE}
# Inputs:
    # sci_dat = A tibble with SCI data. Must include columns user_loc, fr_loc, SCI, and additional columns that specify countries
    # user_country_col = A string that stores the name of the column that specifies the user_loc country (defaults to 'user_country')
    # fr_country_col = A string that stores name of the column that specifies the fr_loc country (defaults to 'fr_country')
    # shapefiles = An sf object that specifies the shapefiles to use
    # region_name_col = A string that stores the name of the column in 'shapefiles' that specifies the region name
    # country = The country code to filter to (matching the format of the data stored in user_country_col)
    # regions = A vector of regions within the country to generate maps for (if NULL will generate maps for every region)
    # output_folder = The folder to save the maps to
    # scale_from_ptile = The maps color buckets are created from scaling up from the Xth percentile of any region pair. This sets that X (defaults to 25).

connectedness <- paste0(mainDirroot,"/data-raw/connected/gadm1_nuts3_counties_gadm1_nuts3_counties_Aug2020.tsv")
# Read in the detailed GADM SCI data (this dataset is quite large and 
# this line of code will likely take a minute or so)
sci_dat <- read_tsv(connectedness)

# nlevels(as.factor(sci_dat$user_loc))
# nlevels(as.factor(sci_dat$fr_loc))

# country_sci <- rename(sci_dat, sci=scaled_sci)  %>% 
#   filter(str_detect(user_loc, "SLV")) %>% 
#   group_by(user_loc ) %>% 
#   summarise(total.count=n(),  
#                count=sum(is.na(sci)),
#                maxsci =max(sci, na.rm=TRUE))  

country_sci <-  sci_dat   %>% 
  filter(str_detect(user_loc, "SLV")) %>% 
  filter(scaled_sci > 1000 ) %>% 
  group_by(user_loc ) %>% 
  summarise( world1000 =n(),  
               maxsciworld1000 =max(scaled_sci, na.rm=TRUE)) %>% 
  
    left_join( sci_dat   %>% 
    filter(str_detect(user_loc, "SLV")) %>% 
    filter(str_detect(fr_loc, "USA")) %>% 
    filter(scaled_sci > 1000 ) %>% 
    group_by(user_loc ) %>% 
    summarise( usa1000 =n(),  
                 maxsciusa1000 =max(scaled_sci, na.rm=TRUE)),
    by = "user_loc") %>% 
   ## Now cleaning PCdode to do the merge
#country_sci2 <-  country_sci  %>% 
              rename(  ADM1_PCODE = user_loc) %>% 
              mutate( ADM1_PCODE = str_replace_all(ADM1_PCODE, "SLV", "SV"))%>% 
              mutate( ADM1_PCODE = ifelse (nchar( ADM1_PCODE) <4, str_replace_all(ADM1_PCODE, "SV", "SV0"), ADM1_PCODE )  )
                              

polyLevel2_sf <- polyLevel2_sf %>% 
  left_join(country_sci ,  by = "ADM1_PCODE")
  

```



```{r echo = FALSE, message = FALSE, warning = FALSE}

country_sci <- rename(sci_dat, sci=scaled_sci)  %>% 
  filter(str_detect(user_loc, "SLV")) %>% 
  filter(str_detect(fr_loc, "SLV")) %>% 
              rename(  ADM1_PCODE = user_loc) %>% 
              mutate( ADM1_PCODE = str_replace_all(ADM1_PCODE, "SLV", "SV"))%>% 
              mutate( ADM1_PCODE = ifelse (nchar( ADM1_PCODE) <4, str_replace_all(ADM1_PCODE, "SV", "SV0"), ADM1_PCODE )  )

## Map for SLV
shapes_in <- polyLevel1_sf 
# levels(as.factor(sci_dat$user_loc))
#levels(as.factor(shapes_in$GID_1 ))
## Clean the code between 2 tables
# shapes_in$fr_loc <- str_remove(shapes_in$GID_1, "_1")
# shapes_in$fr_loc <- str_remove(shapes_in$GID_1, "_1")
## Remove the dot in the mapping code
#shapes_in$user_loc <- str_replace(shapes_in$user_loc1, "\\.$", "")
# shapes_in$fr_loc <- str_replace_all(shapes_in$fr_loc, "[[:punct:]]", "")



# Create measures by scaling up from the chosen percentile of all pairs

scale_from_ptile <- 25
x1 <- quantile(country_sci$sci, scale_from_ptile/100)
  x2 <- x1 * 2
  x3 <- x1 * 3
  x5 <- x1 * 5
  x10 <- x1 * 10
  x25 <- x1 * 25
  x100 <- x1 * 100

  
# for ( curr_region_code in unique(country_sci$user_loc))  {
#   
# ## curr_region_code <- "SLV1" 
# dat <- filter(country_sci, user_loc == curr_region_code)
#     
# # Merge with shape files
# dat_map <- 
#     inner_join(dat, shapes_in, by=c("fr_loc")) %>% 
#     st_as_sf
# 
# # region_name_col  <- "name"
# # fr_country_col  <- "fr_country"
# 
# 
# # Create clean buckets for these levels
# dat_map <- dat_map %>% 
#       mutate(sci_bkt = case_when(
#         sci < x1 ~ str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#         sci < x2 ~ "1-2x",
#         sci < x3 ~ "2-3x",
#         sci < x5 ~ "3-5x",
#         sci < x10 ~ "5-10x",
#         sci < x25 ~ "10-25x",
#         sci < x100 ~ "25-100x",
#         sci >= x100 ~ ">= 100x")) %>% 
#       mutate(sci_bkt = factor(sci_bkt, levels=c(str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#                                                 "1-2x", "2-3x", "3-5x", "5-10x", "10-25x", "25-100x", ">= 100x")))
#     
# # Get the map of the region you are in
# curr_region_outline <- dat_map %>% 
#     filter(fr_loc == curr_region_code)
#     
# curr_region_name <- curr_region_outline$NAME_1
# 
# 
# 
# # Plot the data
# p <- ggplot(st_transform(dat_map, "+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs")) +
#       geom_sf(aes(fill = sci_bkt), 
#               colour="#DADADA", 
#               size=0.25) +
#       geom_sf(data=curr_region_outline, 
#               fill="#A00000", 
#               colour="#A00000", 
#               size=0.5) +
#   
# 
#       scale_fill_brewer(palette = "GnBu", na.value="#F5F5F5", drop=FALSE) +
#   
#       labs(title = paste0("Internal Social Connectedness Index for department: ",curr_region_name) ,
#            subtitle = "El Salvador, as of August 2020",
#            x = "",
#            y = "",
#            fill = "SCI", 
#            caption = "Source: Facebook Data For Good ") + 
#       
#       unhcRstyle::unhcr_theme(base_size = 12)  + ## Insert UNHCR Style
#       theme(#panel.grid.major = element_line(color = gray(.5),  linetype = "dashed", size = 0.5),
#             panel.grid.major = element_blank(),
#             panel.background = element_rect(fill = "aliceblue"),
#             axis.text.x  = element_blank(),
#             axis.text.y  = element_blank(),
#             legend.title = element_blank(), 
#             legend.text  = element_text(size = 8),
#             legend.key.size = unit(0.8, "lines"),
#             legend.position = "bottom", 
#             legend.box = "horizontal"
#             ) # +
#       #guides(fill = guide_legend(nrow = 1, title.hjust = 0.5))
# 
# print(p)
#   
# }

```




```{r echo = FALSE, message = FALSE, warning = FALSE}

country_sci <- rename(sci_dat, sci=scaled_sci)  %>% 
  filter(str_detect(user_loc, "SLV")) %>% 
  filter(str_detect(fr_loc, c("SLV","GTM", "HND")))


#names(gadm1)
## Map for SLV
shapes_in <- gadm1 %>% 
  st_transform(crs=4326)  %>% 
 # st_transform(crs("+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"))  %>% 
  filter(GID_0 %in% c("SLV","GTM", "HND", "MEX")) 

shapes_in <- shapes_in[ , c("GID_1", "NAME_1", "geometry")]

  

# levels(as.factor(country_sci$fr_loc))
#levels(as.factor(shapes_in$GID_1 ))
# levels(as.factor(shapes_in$NAME_1 ))
## Clean the code between 2 tables
shapes_in$fr_loc <- str_remove(shapes_in$GID_1, "_1")
## Remove the dot in the mapping code
#shapes_in$user_loc <- str_replace(shapes_in$user_loc1, "\\.$", "")
shapes_in$fr_loc <- str_replace_all(shapes_in$fr_loc, "[[:punct:]]", "")
#levels(as.factor(shapes_in$fr_loc ))


## Check what you get
##plot(sf::st_geometry(shapes_in))

# Create measures by scaling up from the chosen percentile of all pairs

scale_from_ptile <- 25
x1 <- quantile(country_sci$sci, scale_from_ptile/100)
x2 <- x1 * 2
x3 <- x1 * 3
x5 <- x1 * 5
x10 <- x1 * 10
x25 <- x1 * 25
x100 <- x1 * 100

  
# for ( curr_region_code in unique(country_sci$user_loc))  {
#   
# ## curr_region_code <- "SLV1" 
# dat <- filter(country_sci, user_loc == curr_region_code)
#     
# # Merge with shape files
# dat_map <- 
#     inner_join(dat, shapes_in, by=c("fr_loc")) %>% 
#     st_as_sf  %>% 
#     mutate(sci_bkt = case_when(
#         sci < x1 ~ str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#         sci < x2 ~ "1-2x",
#         sci < x3 ~ "2-3x",
#         sci < x5 ~ "3-5x",
#         sci < x10 ~ "5-10x",
#         sci < x25 ~ "10-25x",
#         sci < x100 ~ "25-100x",
#         sci >= x100 ~ ">= 100x")) %>% 
#     mutate(sci_bkt = factor(sci_bkt, levels=c(str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#                                                 "1-2x", "2-3x", "3-5x", "5-10x", "10-25x", "25-100x", ">= 100x")))
#     
# # Get the map of the region you are in
# curr_region_outline <-
#     inner_join(dat, shapes_in, by=c("user_loc"="fr_loc")) %>% 
#     filter(user_loc == curr_region_code) %>% 
#     head(1)  %>% 
#    st_as_sf() 
#     
# curr_region_name <-  curr_region_outline$NAME_1 
# ##plot(sf::st_geometry(curr_region_outline))
# 
# 
# # Plot the data
# p <- ggplot(  ) +
#       geom_sf(data=shapes_in, 
#               colour="black", 
#               size=0.1) +
#       geom_sf(data=dat_map, aes(fill = sci_bkt), 
#               colour="#DADADA", 
#               size=0.15) +
#       geom_sf(data=curr_region_outline, 
#               fill="#A00000", 
#               colour="#A00000", 
#               size=0.5) +
#   
# 
#       scale_fill_brewer(palette = "GnBu", na.value="#F5F5F5", drop=FALSE) +
#   
#       labs(title = paste0("International Social Connectedness Index for department: ",curr_region_name) ,
#            subtitle = "El Salvador to Honduras, Guatemala & Mexico, as of August 2020",
#            x = "",
#            y = "",
#            fill = "SCI", 
#            caption = "Source: Facebook Data For Good ") + 
#       
#       unhcRstyle::unhcr_theme(base_size = 12)  + ## Insert UNHCR Style
#       theme(#panel.grid.major = element_line(color = gray(.5),  linetype = "dashed", size = 0.5),
#             panel.grid.major = element_blank(),
#             panel.background = element_rect(fill = "aliceblue"),
#             axis.text.x  = element_blank(),
#             axis.text.y  = element_blank(),
#             legend.title = element_blank(), 
#             legend.text  = element_text(size = 8),
#             legend.key.size = unit(0.8, "lines"),
#             legend.position = "bottom", 
#             legend.box = "horizontal"
#             ) # +
#       #guides(fill = guide_legend(nrow = 1, title.hjust = 0.5))
# 
# print(p)
#   
# }

```



```{r echo = FALSE, message = FALSE, warning = FALSE}

country_sci <- rename(sci_dat, sci=scaled_sci)  %>% 
  filter(str_detect(user_loc, "SLV")) %>% 
  filter(str_detect(fr_loc, c("SLV","GTM", "HND", "MEX", "USA")))


#names(gadm1)
## Map for SLV
shapes_in <- gadm1 %>% 
  st_transform(crs=4326)  %>% 
 # st_transform(crs("+proj=robin +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs"))  %>% 
  filter(GID_0 %in% c("SLV","GTM", "HND", "MEX")) 

shapes_in <- shapes_in[ , c("GID_1", "NAME_1", "geometry")]

  

# levels(as.factor(country_sci$fr_loc))
#levels(as.factor(shapes_in$GID_1 ))
# levels(as.factor(shapes_in$NAME_1 ))
## Clean the code between 2 tables
shapes_in$fr_loc <- str_remove(shapes_in$GID_1, "_1")
## Remove the dot in the mapping code
#shapes_in$user_loc <- str_replace(shapes_in$user_loc1, "\\.$", "")
shapes_in$fr_loc <- str_replace_all(shapes_in$fr_loc, "[[:punct:]]", "")
#levels(as.factor(shapes_in$fr_loc ))


## Merged GADm1 & US Counties from Tigris
counties_map$fr_loc <- counties_map$GID_1
shapes_in <- rbind(shapes_in, counties_map)

## Check what you get
##plot(sf::st_geometry(shapes_in))

# Create measures by scaling up from the chosen percentile of all pairs

scale_from_ptile <- 25
x1 <- quantile(country_sci$sci, scale_from_ptile/100)
x2 <- x1 * 2
x3 <- x1 * 3
x5 <- x1 * 5
x10 <- x1 * 10
x25 <- x1 * 25
x100 <- x1 * 100

  
# for ( curr_region_code in unique(country_sci$user_loc))  {
#   
# ## curr_region_code <- "SLV1" 
# dat <- filter(country_sci, user_loc == curr_region_code)
#     
# # Merge with shape files
# dat_map <- 
#     inner_join(dat, shapes_in, by=c("fr_loc")) %>% 
#     st_as_sf  %>% 
#     mutate(sci_bkt = case_when(
#         sci < x1 ~ str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#         sci < x2 ~ "1-2x",
#         sci < x3 ~ "2-3x",
#         sci < x5 ~ "3-5x",
#         sci < x10 ~ "5-10x",
#         sci < x25 ~ "10-25x",
#         sci < x100 ~ "25-100x",
#         sci >= x100 ~ ">= 100x")) %>% 
#     mutate(sci_bkt = factor(sci_bkt, levels=c(str_interp("< 1x (Country ${scale_from_ptile}th percentile)"),
#                                                 "1-2x", "2-3x", "3-5x", "5-10x", "10-25x", "25-100x", ">= 100x")))
#     
# # Get the map of the region you are in
# curr_region_outline <-
#     inner_join(dat, shapes_in, by=c("user_loc"="fr_loc")) %>% 
#     filter(user_loc == curr_region_code) %>% 
#     head(1)  %>% 
#    st_as_sf() 
#     
# curr_region_name <-  curr_region_outline$NAME_1 
# ##plot(sf::st_geometry(curr_region_outline))
# 
# 
# # Plot the data
# p <- ggplot(  ) +
#       geom_sf(data=shapes_in, 
#               colour="black", 
#               size=0.1) +
#       geom_sf(data=dat_map, aes(fill = sci_bkt), 
#               colour="#DADADA", 
#               size=0.15) +
#       geom_sf(data=curr_region_outline, 
#               fill="#A00000", 
#               colour="#A00000", 
#               size=0.5) +
#   
# 
#       scale_fill_brewer(palette = "GnBu", na.value="#F5F5F5", drop=FALSE) +
#   
#       labs(title = paste0("Interntional Social Connectedness Index for department: ",curr_region_name) ,
#            subtitle = "El Salvador to USA, Honduras, Guatemala & Mexico, as of August 2020",
#            x = "",
#            y = "",
#            fill = "SCI", 
#            caption = "Source: Facebook Data For Good ") + 
#       
#       unhcRstyle::unhcr_theme(base_size = 12)  + ## Insert UNHCR Style
#       theme(#panel.grid.major = element_line(color = gray(.5),  linetype = "dashed", size = 0.5),
#             panel.grid.major = element_blank(),
#             panel.background = element_rect(fill = "aliceblue"),
#             axis.text.x  = element_blank(),
#             axis.text.y  = element_blank(),
#             legend.title = element_blank(), 
#             legend.text  = element_text(size = 8),
#             legend.key.size = unit(0.8, "lines"),
#             legend.position = "bottom", 
#             legend.box = "horizontal"
#             ) # +
#       #guides(fill = guide_legend(nrow = 1, title.hjust = 0.5))
# 
# print(p)
#   
# }

```





## Public Services

```{r}
# https://geocompr.robinlovelace.net/location.html

# library(osmdata)
# parks = opq(bbox = "el_slavador") %>% 
#   add_osm_feature(key = "leisure", value = "park") %>% 
#   osmdata_sf()

```


#  Environment:

## Agricultural drought frequency Index 

[FAO-ASI](http://www.fao.org/giews/earthobservation/country/index.jsp?code=SLV&lang=en))

Historic Agricultural Drought Frequency Maps depict the frequency of severe drought in areas where 30 percent/50 percent of the cropland has been affected. The historical frequency of severe droughts (as defined by ASI) is based on the entire ASI times series (1984-2020).


```{r}

# gdal_translate -of Gtiff  -outsize 5000 5000 "WMS:https://io.apps.fao.org/geoserver/wms/ASIS/HDF/v2?SERVICE=WMS&VERSION=1.1.1&REQUEST=GetMap&LAYERS=HDF&SRS=EPSG:4326&BBOX=-90.3159,12.8091,-87.4539,14.6022" asi2_slv.tiff


## save locallly a geotiff from the WMS server
# system("gdal_translate -of Gtiff  -outsize 5000 5000 \"WMS:https://io.apps.fao.org/geoserver/wms/ASIS/HDF/v2?SERVICE=WMS&VERSION=1.1.1&REQUEST=GetMap&LAYERS=HDF&SRS=EPSG:4326&BBOX=-90.3159,12.8091,-87.4539,14.6022\" /data-raw/fao/asi2_slv.tiff")  

# load raster in an R object called 'DEM'
ASIpath <- paste0(mainDirroot,"/data-raw/fao/asi2_slv.tiff")
ASI <- raster(ASIpath) 
# 
# GDALinfo(ASIpath)
# 
# range(values(ASI)[,1])
# 
# # turn raster into data.frame and copy the colortable
# ASI.df <- data.frame(rasterToPoints(ASI))
# names(ASI.df)[3] <-  "value"
# 
# coltab <- colortable(ASI)
# coltab <- coltab[(unique(ASI))+1]
# # give the colors their apropriate names:
# names(coltab) <- 0:(length(coltab) - 1) 
# 
# # only define the colors used in the raster image
# from <- min(ASI.df[[3]], na.rm = T)+1 
# to <- max(ASI.df[[3]], na.rm = T)+1
# used_cols <- coltab[from:to] 
# 
# # plot:
# ggplot(ASI.df, 
#        maxpixels = 5e5) +
#  # facet_wrap(~ variable) +
#   geom_tile(aes(x= x, y = y, fill = value)) +
#   scale_fill_gradientn(colours=used_cols) +
#   coord_equal()
# 
# ggplot(ASI, maxpixels=5e5) + 
#   geom_tile(aes(fill = value)) +
#   facet_wrap(~ variable) +
#   scale_fill_gradientn(colours=coltab, guide=FALSE) +
#   coord_equal()
# 
# ASI.img <- melt(ASI)
# colTab <- attr(ASI, "legend")@colortable
# names(colTab) <- 0:(length(colTab) - 1)
# valTab <- sort(unique(ASI.df[[3]]))
# 
# plot(ASI, 
#      main="Historic Agricultural Drought Frequency")

wmshist <- "https://io.apps.fao.org/geoserver/wms/ASIS/HDF/v2?version=1.3.0"
layerhist <- "HDF"



# wmsfao <- "https://io.apps.fao.org/geoserver/wms/ASIS/HDF/v1?version=1.3.0"
# 
# layer_crop_season1_30p <- "HDF_C_S1_LA30:ASIS:asis_hdf_c"
# layer_crop_season1_50p <- "HDF_C_S1_LA50:ASIS:asis_hdf_c"
# layer_crop_season2_30p <- "HDF_C_S2_LA30:ASIS:asis_hdf_c"
# layer_crop_season2_50p <- "HDF_C_S2_LA50:ASIS:asis_hdf_c"


```


```{r}
# this example from the tutorial works
leaflet() %>% 
  addTiles() %>% 
  setView(lat = 13.7167, lng = -88.8602, zoom = 9) %>% 
  addWMSTiles(
    wmshist,
    layers = layerhist,
    options = WMSTileOptions(format = "image/png", transparent = TRUE) 
)


# wmshist1 <- PreprocessWMS(url = LoadWMSurl(provider = "OIVA", service = "Corine"))
# 
# test <- sorvi::GetWMSraster(wmshist1, 
#                             layerhist, 
#                             extent = polyLevel2_sf , 
#                             resolution = 25)
```


```{r}
# # this example from the tutorial works
# leaflet() %>% 
#   addTiles() %>% 
#   setView(lat = 13.7167, lng = -88.8602, zoom = 9) %>% 
#   addWMSTiles(
#     wmsfao,
#     layers = layer_crop_season1_30p,
#     options = WMSTileOptions(format = "image/png", transparent = TRUE) 
# )
```
 
```{r}
# # this example from the tutorial works
# leaflet() %>% 
#   addTiles() %>% 
#   setView(lat = 13.7167, lng = -88.8602, zoom = 9) %>% 
#   addWMSTiles(
#     wmsfao,
#     layers = layer_crop_season1_50p,
#     options = WMSTileOptions(format = "image/png", transparent = TRUE) 
# )
```
 
```{r}
# # this example from the tutorial works
# leaflet() %>% 
#   addTiles() %>% 
#   setView(lat = 13.7167, lng = -88.8602, zoom = 9) %>% 
#   addWMSTiles(
#     wmsfao,
#     layers = layer_crop_season2_50p,
#     options = WMSTileOptions(format = "image/png", transparent = TRUE) 
# )
```
 
```{r}
# # this example from the tutorial works
# leaflet() %>% 
#   addTiles() %>% 
#   setView(lat = 13.7167, lng = -88.8602, zoom = 9) %>% 
#   addWMSTiles(
#     wmsfao,
#     layers = layer_crop_season2_30p,
#     options = WMSTileOptions(format = "image/png", transparent = TRUE) 
# )
# 

```

## Climatic Natural Risk - Flood and Cyclone  (Prevention Web)
## Geologic Natural Risk - Earthquake and Volcano  (Prevention Web)
  

# Clustering analysis of different areas

## Composite indicators



The __polarity__ of a sub-indicator is the sign of the relationship between the indicator and the phenomenon to be measured (e.g., in a well-being index, "GDP per capita" has 'positive' polarity and "Unemployment rate" has 'negative' polarity). In this case, we have 2 options for such directional adjustments:  

 * "Negative (the higher score, the more severe)"
 * "Positive (the higher score, the less severe)"
 
This component is accounted for during the normalization process below.


__Data Normalization__ allows for Adjustments of distribution (similar range of variation) and scale (common scale) of sub-indicators that may reflect different units of measurement and different ranges of variation. 

Due to the structure of the indicators, distinct approaches of normalization shall be considered in order to avoid having zero value that would create issues for geometric means aggregation. Different normalization methods are available through the function `normalise_ci` and lead to different results:

 * A z-score approach `method = 1` (Imposes a distribution with mean zero and variance 1). Standardized scores which are below average will become negative, implying that further geometric aggregation will be prevented.
 
 * A min-max approach `method = 2` (same range of variation [0,1]) but not same variance).This method is very sensitive to extreme values / outliers
 
 * A ranking method `method = 3`. Scores are replaced by ranks – e.g. the highest score receives the first ranking position (rank 1).


```{r}
#names(polyLevel2_sf)

 ## - "ADM2_PCODE" "ADM2_ES" "ADM1_ES"
# ## Is normalised with percentage  
#  "percent_elderly" ,   "percent_under5","percent_reproductive_women" ,  "percent_youth",                                             
# 
# #### Violence needs to be normalised 
# "battles",  "explosions_remote_violence" ,  "protests",                          
#  "strategic_developments",  "violence_against_civilians", 

## Subventype
##"riots",
# "abduction_forced_disappearance","armed_clash",                       
#  "arrests" ,   "attack",   "change_to_group_activity",          
#  "grenade"  , "peaceful_protest" ,   "mob_violence" ,                    
#  "violent_demonstration" , "protest_with_intervention" ,  "other", 
#  "remote_explosive_landmine_ied" , "sexual_violence" ,                   "looting_property_destruction" , "disrupted_weapons_use" ,  
# "excessive_force_against_protesters",
# 
# ## Connectedness needs to normalised ans scaled
#  "world1000" , "maxsciworld1000", "usa1000",  "maxsciusa1000",  
# ## Realtive Wealth
# "rwi_avg", "rwi_sd","rwi_max",  "rwi_min"  

subindicator <- data.frame(
                 ## var name
                 Name = c( "percent_elderly" ,   "percent_under5","percent_reproductive_women" ,  "percent_youth", 
                                      "battles",  "explosions_remote_violence" ,  "protests",  "strategic_developments",  "violence_against_civilians", 
                                     
                                     "world1000" , "maxsciworld1000", "usa1000",  "maxsciusa1000",  
                                     "rwi_avg", "rwi_sd","rwi_max",  "rwi_min"),
                 ## var name
                 Label = c( "Percent of elderly" ,   "Percent of under5","Percent of reproductive_women" ,  "Percent of youth", 
                                      "Battles",  "Explosions, Remote Violence" ,  "Protests",  "Strategic Developments",  "Violence Against Divilians", 
                                     
                                     "Connectedness World1000" , "Connectedness max world1000", "Connectedness usa1000",  "Connectedness max  usa1000",  
                                     "Relative Wealth Avg", "Relative Wealth  sd","Relative Wealth max",  "Relative Wealth min"),
                 ## Subdimension          
                 Dimension= c("social","social","social","social",
                              "social","social","social","social","social",
                              
                              "economic","economic","economic","economic",
                              "economic","economic","economic","economic"),

                 # Calculation - informing the normalisation approach "binary", "scored", "value"
                 Calculation = c("value","value","value","value",
                                 "value","value","value","value","value",
                                 "value","value","value","value",
                                 "scored","scored","scored","scored"),
                 ## "Negative (the higher score, the more severe)"
                 Polarity = c("NEG", "NEG", "NEG","NEG",
                              "NEG", "NEG", "NEG","NEG","NEG",
                              "NEG", "NEG", "NEG","NEG",
                              "NEG", "NEG", "NEG","NEG"),
                 ## Direction
                 Direction = c(1,1,1,1,
                         1,1,1,1,1,
                         1,1,1,1,
                         1,1,1,1)  
                 )


indic <- polyLevel2_sf %>%
       st_drop_geometry() %>%
       as.data.frame()  %>%
   #mutate( row.names() = ADM2_PCODE) %>%
  
  dplyr::select(  
  ## Is normalised with percentage  
   percent_elderly ,   percent_under5,percent_reproductive_women ,  percent_youth,
  #### Violence needs to be normalised 
  battles,  explosions_remote_violence ,  protests,                          
   strategic_developments,  violence_against_civilians,   riots,
  abduction_forced_disappearance,armed_clash,                       
   arrests ,   attack,   change_to_group_activity,          
   grenade  , peaceful_protest ,   mob_violence ,                    
   violent_demonstration , protest_with_intervention ,  other, 
   remote_explosive_landmine_ied , sexual_violence ,
  looting_property_destruction , disrupted_weapons_use ,  
  excessive_force_against_protesters,
  
  ## Connectedness needs to normalised ans scaled
   world1000 , maxsciworld1000, usa1000,  maxsciusa1000,  
  ## Realtive Wealth
  rwi_avg, rwi_sd,rwi_max,  rwi_min )

row.names(indic) <- polyLevel2_sf$ADM2_PCODE

indic2 <- sapply(indic, as.numeric)

# to replace multiple values in a data frame, looping through all columns might help.
for (j in 1:nrow(indic2)) { 
  for (i in 1:ncol(indic2)) {
    indic2[j, i] <- ifelse(  is.na(indic2[j, i]), 0, indic2[j, i])
  }
}

### Remove var when standard deviation is 0
#indic2 <- indic2[, sapply(indic2, function(x) { sd(x) != 0} )]


## Transform this object as a matrix and inject location name as row.names
indic.matrix <- as.matrix(indic2)
row.names(indic.matrix) <- polyLevel2_sf$ADM2_PCODE
                    
```

__Data Normalization__ allows for Adjustments of distribution (similar range of variation) and scale (common scale) of sub-indicators that may reflect different units of measurement and different ranges of variation. 

Due to the structure of the indicators, distinct approaches of normalization shall be considered in order to avoid having zero value that would create issues for geometric means aggregation. Different normalization methods are available through the function `normalise_ci` and lead to different results:

 * A z-score approach `method = 1` (Imposes a distribution with mean zero and variance 1). Standardized scores which are below average will become negative, implying that further geometric aggregation will be prevented.
 
 * A min-max approach `method = 2` (same range of variation [0,1]) but not same variance).This method is very sensitive to extreme values/outliers
 
 * A ranking method `method = 3`. Scores are replaced by ranks – e.g. the highest score receives the first ranking position (rank 1).



```{r, message=FALSE, warning=FALSE}
## Retrieve polarity from dictionnary

## Display the table

## Case 1
subindicator.scored <- subindicator[ subindicator$Calculation == "scored", ]
var.scored <- as.character(subindicator.scored[ , c("Name") ])
indic.matrix.scored <- indic.matrix[ , var.scored ]

 
# indic.matrix.scored.obj <- Compind::normalise_ci(indic.matrix.scored,
#                                      c(1:ncol(indic.matrix.scored)),
#                                      polarity =  as.character(subindicator.scored$Polarity ),
#                                      method = 1)

## Case 2
subindicator.value <- subindicator[ subindicator$Calculation == "value", ]
var.value <- as.character(subindicator.value[ , c("Name") ])
indic.matrix.value <- indic.matrix[ , var.value ]
indic.matrix.value.obj <- Compind::normalise_ci(indic.matrix.value,
                                     c(1:ncol(indic.matrix.value)),
                                     polarity =  as.character(subindicator.value$Polarity ),
                                     method = 2)

## Case 3
# subindicator.binary <- subindicator[ subindicator$Calculation == "binary", ]
# var.binary <- as.character(subindicator.binary[ , c("Name") ])
# indic.matrix.binary <- indic.matrix[ , var.binary ]
# indic.matrix.binary.obj <- Compind::normalise_ci(indic.matrix.binary,
#                                      c(1:ncol(indic.matrix.binary)),
#                                      polarity =  as.character(subindicator.binary$Polarity ),
#                                      method = 3)

## Binding this together so that we have the full normalised matrix
indic.matrix.norm <- cbind(#indic.matrix.scored.obj$ci_norm,
                           indic.matrix.scored,
                           indic.matrix.value.obj$ci_norm #, indic.matrix.binary.obj$ci_norm
                           )

## Clean the work environment  
rm( subindicator.unique,
 #  subindicator.value, var.value , indic.matrix.value, indic.matrix.value.obj,
   subindicator.binary, var.binary , indic.matrix.binary,# indic.matrix.binary.obj,
   subindicator.scored, var.scored , indic.matrix.scored, indic.matrix.scored.obj   )

```


### Correlation analysis

The investigation of the structure of simple indicators can be done by means of correlation analysis. 

We will check such correlation first within each dimension, using the [ggcorrplot](http://www.sthda.com/english/wiki/ggcorrplot) package. An alternative approach to better visualize correlation between indicators is to [represent them through a network](http://sachaepskamp.com/files/Cookbook.html#customizing-graphs) with the [ggpraph](https://github.com/thomasp85/ggraph) package. 

'
```{r, message=FALSE, warning=FALSE, fig.height= 8, fig.width= 8}
##  Frame with all dimensions
dimensions <- as.data.frame( unique(subindicator[ ,c( "Dimension" )]))
names(dimensions)[1] <- "Dimension"

## Creating severity subindice on each dimensions with Data Envelopment analysis #####

for (i in 1:nrow(dimensions)) {
  # i <- 1
  # ## looping around dimensions
 this.dimension <- as.character(dimensions[i,1])
  ## subset related indicator names
  this.indicators <- as.character(subindicator[ subindicator$Dimension == this.dimension,
                                                        c("Name") ])
  this.indicators.label <- as.character(subindicator[ subindicator$Dimension == this.dimension,
                                                        c("Label") ])
  ##subset matrix & df
  this.indic.matrix.norm <- indic.matrix[ , this.indicators]
  this.indic.df <- indic2[ , this.indicators]

### Check correlation
corr.matrix <- cor(this.indic.matrix.norm, method = "pearson",  use = "pairwise.complete.obs")
  
  
## replace with Label inside the matrix
corr.matrix1 <- corr.matrix
rownames(corr.matrix1) <- as.character(subindicator[subindicator$Name %in% rownames(corr.matrix), c("Label")])
colnames(corr.matrix1) <- as.character(subindicator[subindicator$Name %in% colnames(corr.matrix), c("Label")])

#plot1 <- ggcorrplot(corr.matrix1 ,
plot1 <- 		ggcorrplot::ggcorrplot(corr.matrix1 ,
                    method = "circle",
                    hc.order = TRUE,
                    type = "upper") +
 # labs(title = paste0( "Severity Indicators for ",this.dimension ),
  labs(title = paste0( "Severity Indicators for " ),
       subtitle = "Identified Correlation between indicators",
       caption = "Correlation level = dot size, Positive Correlation  = Red - Negative = Blue",
       x = NULL, y = NULL) +
  unhcRstyle::unhcr_theme() +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 7),
         strip.text.x = element_text(size = 7),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = "top",
         legend.box = "horizontal",
         legend.text = element_text(size = 9),
         panel.grid.major.x = element_line(color = "#cbcbcb"),
         panel.grid.major.y = element_line(color = "#cbcbcb")) 
print(plot1)


plot2 <- qgraph::qgraph(cor(this.indic.matrix.norm),
     # shape = "circle",
     # posCol = "darkgreen",
     # negCol = "darkred",
     # threshold = "bonferroni", #The threshold argument can be used to remove edges that are not significant.
     # sampleSize = nrow(scores.this.norm),
     # graph = "glasso",
       esize = 35, ## Size of node
       vsize = 6,
       vTrans = 600,
       posCol = "#003399", ## Color positive correlation Dark powder blue
       negCol = "#FF9933", ## Color negative correlation Deep Saffron
       alpha = 0.05,
       cut = 0.4, ## cut off value for correlation
       maximum = 1, ## cut off value for correlation
       palette = 'pastel', # adjusting colors
       borders = TRUE,
       details = FALSE,
       layout = "spring",
      # nodeNames = this.indicators.label ,
       legend.cex = 0.4,
     #  title = paste0("Correlations Network for severity indicators related ",this.dimension ),   
     title = paste0("Correlations Network for severity indicators related "),
       line = -2,
       cex.main = 2)
print(plot2)
 }

```
 


### Consistency between indicators

Cronbach’s alpha, (or coefficient alpha), developed by Lee Cronbach in 1951, measures reliability (i.e.  how well a test measures what it should: measure of the stability of test scores), or [internal consistency](https://www.statisticshowto.datasciencecentral.com/internal-consistency/). 

As a rule of thumbs, a score of more than 0.7 indicates an acceptable level of consistency: 

 * A high level for alpha may mean that all indicators are highly correlated (meaning we have redundant indicators representing the same thing...). 
 * A low value for alpha may mean that there are not enough indicators or that the indicators are poorly interrelated.

'
```{r message=FALSE, warning=FALSE, comment=NA}
Cronbach.this <- psych::alpha(this.indic.matrix.norm, check.keys = TRUE)

cat(paste0("The Cronbach Alpha measure of consistency for this combination of indicators is  ", round(Cronbach.this$total$std.alpha, 2), "\n." ) )
```


## Aggregation & Weighting

For weighting, the main issue to address is related to the concept of **compensability**. Namely the question is to know to what extent can we accept that the high score of an indicator go to compensate the low score of another indicator? This problem of compensability is intertwined with the issue of attribution of weights for each sub-indicator in order to calculate the final aggregation. 

We can foresee that using _"equal weight"_ (all indicators account for the same in the final index) and _"arithmetic aggregation"_ (all indicators are substituable) is unlikely to depict the complex issue of Humanitarian Severity and is likely to comes with the risk of misrepresenting the reality. 

Various methods are available within the [Compind package](https://cran.r-project.org/web/packages/Compind/index.html) are described below. This R package is also available through a [ShinyApp](https://fvidoli.shinyapps.io/compind_app/). We will then share the code to use them based on our example.

### Benefit of the Doubt approach (BoD)

 This method is the application of Data Envelopment Analysis (DEA) to the field of composite indicators. It was originally proposed by Melyn and Moesen (1991) to evaluate macroeconomic performance.  ACAPS has prepared an excellent note on [The use of data envelopment analysis to calculate priority scores in needs assessments](https://www.acaps.org/sites/acaps/files/resources/files/the_use_of_data_envelopment_analysis_to_calculate_priority_scores_in_needs_assessments_july_2015.pdf).
 
 BoD approach offers several advantages:
 
 *  Weights are endogenously determined by the observed performances and benchmark is not based on theoretical bounds, but it’s a linear combination of the observed best performances.
 
 *  Principle is easy to communicate:  since we are not sure about the right weights, we look for ”benefit of the doubt” weights (such that your overall relative performance index is as high as possible).

```{r, message=FALSE, warning=FALSE}
CI_BoD_estimated = Compind::ci_bod(this.indic.matrix.norm,
                          indic_col = (1:ncol(this.indic.matrix.norm)))

ci_bod_est <- as.data.frame( CI_BoD_estimated$ci_bod_est)
names(ci_bod_est) <- "Benef_Doubt"
```


### Directional Benefit of the Doubt (D-BoD)

Directional Benefit of the Doubt (D-BoD) model enhances non-compensatory property by introducing directional penalties in a standard BoD model in order to consider the preference structure among simple indicators. This method is described in the article [Enhancing non compensatory composite indicators: a directional proposal](https://www.aisre.it/images/old_papers/AISRE.pdf).

```{r, message=FALSE, warning=FALSE}
## Endogenous weight - no zero weight --
CI_Direct_BoD_estimated <-  Compind::ci_bod_dir(this.indic.matrix.norm,
                                       indic_col = (1:ncol(this.indic.matrix.norm)),
                                       dir = as.numeric(subindicator[subindicator$Dimension == this.dimension , c("Direction")] ))

ci_bod_dir_est <- data.frame(CI_Direct_BoD_estimated$ci_bod_dir_est)
names(ci_bod_dir_est) <- "Benef_Doubt_Dir"
```

### Robust Benefit of the Doubt approach (RBoD) 

 This method is the robust version of the BoD method. It is based on the concept of the expected minimum input function of order-m so "in place of looking for the lower boundary of the support of F, as was typically the case for the full-frontier (DEA or FDH), the order-m efficiency score can be viewed as the expectation of the maximal score, when compared to m units randomly drawn from the population of units presenting a greater level of simple indicators", Daraio and Simar (2005). This method is described with more detail in the article [Robust weighted composite indicators by means of frontier methods with an application to European infrastructure endowment](http://sa-ijas.stat.unipd.it/sites/sa-ijas.stat.unipd.it/files/06.pdf).

```{r, message=FALSE, warning=FALSE}
CI_RBoD_estimated <-  Compind::ci_rbod(this.indic.matrix.norm,
                              indic_col = (1:ncol(this.indic.matrix.norm)),
                              M = 20,  #The number of elements in each sample.
                              B = 200) #The number of bootstap replicates.

ci_rbod_est <- data.frame(CI_RBoD_estimated$ci_rbod_est)
names(ci_rbod_est) <- "Benef_Doubt_Rob"
```



```{r, message=FALSE, warning=FALSE}
### Benefit of the Doubt approach (BoD) index with constraints on weights 

# This method allows for constraints (so that there is none not valued) and with a penalty as proposed by Mazziotta - Pareto (also adopted by the Italian National Institute of Statistics). This method is described in the article [Geometric mean quantity index numbers with Benefit-of-the-Doubt weights](https://www.sciencedirect.com/science/article/abs/pii/S0377221716305938) 
# CI_BoD_MPI_estimated = Compind::ci_bod_constr_mpi(this.indic.matrix.norm,
#                                           indic_col = (1:ncol(this.indic.matrix.norm)),
#                                           up_w = 1,
#                                           low_w = 0.1,
#                                           penalty = "POS")
# 
# ci_bod_constr_est_mpi <- data.frame(CI_BoD_MPI_estimated$ci_bod_constr_est_mpi)
# names(ci_bod_constr_est_mpi) <- "Benef_Doubt_Cons"
```


### Factor analysis 

 This method groups together simple indicators to estimate a composite indicator that captures as much as possible of the information common to individual indicators.
 

```{r, message=FALSE, warning=FALSE}
##  Doing PCA with ci_factor.R
# If method = "ONE" (default) the composite indicator estimated values are equal to first component scores;
# if method = "ALL" the composite indicator estimated values are equal to component score multiplied by its proportion variance;
# if method = "CH" it can be choose the number of the component to take into account.
dimfactor <- ifelse(ncol(this.indic.matrix.norm) > 2, 3, ncol(this.indic.matrix.norm))
CI_Factor_estimated <-  Compind::ci_factor(this.indic.matrix.norm,
                                  indic_col = (1:ncol(this.indic.matrix.norm)),
                                  method = "CH",  # if method = "CH" it can be choose the number of the component to take into account.
                                  dim = dimfactor)
ci_factor_est <- data.frame( CI_Factor_estimated$ci_factor_est)
names(ci_factor_est) <- "Factor"
```

### Mean-Min Function (MMF) 
 
 This method is an intermediate case between arithmetic mean, according to which no unbalance is penalized, and min function, according to which the penalization is maximum. It depends on two parameters that are respectively related to the intensity of penalization of unbalance (alpha) and intensity of complementarity (beta) among indicators. "An unbalance adjustment method for development indicators"

```{r, message=FALSE, warning=FALSE}
CI_mean_min_estimated <- Compind::ci_mean_min(this.indic.matrix.norm,
                                     indic_col = (1:ncol(this.indic.matrix.norm)),
                                     alpha = 0.5,  #intensity of penalization of unbalance  (alpha)
                                     beta = 1) # intensity of complementarity (beta) among indicators

ci_mean_min_est <- data.frame( CI_mean_min_estimated$ci_mean_min_est)
names(ci_mean_min_est) <- "Mean_Min"
```

### Geometric aggregation 

This method uses the geometric mean to aggregate the single indicators and therefore allows to bypass the full compensability hypothesis using geometric mean. Two weighting criteria are possible: EQUAL: equal weighting and BOD: Benefit-of-the-Doubt weights following the Puyenbroeck and Rogge (2017) approach.

```{r, message=FALSE, warning=FALSE}

CI_Geom_estimated = Compind::ci_geom_gen(this.indic.matrix.norm,
                                indic_col = (1:ncol(this.indic.matrix.norm)),
                                meth = "EQUAL",
                                ## "EQUAL" = Equal weighting set, "BOD" = Benefit-of-the-Doubt weighting set.
                                up_w = 1,
                                low_w = 0.1,
                                bench = 1)
# Row number of the benchmark unit used to normalize the data.frame x.

ci_mean_geom_est <- data.frame( CI_Geom_estimated$ci_mean_geom_est)
names(ci_mean_geom_est) <- "Mean_Geom"
```

### Mazziotta-Pareto Index (MPI)
 
This method is  is a non-linear composite index method which transforms a set of individual indicators in standardized variables and summarizes them using an arithmetic mean adjusted by a "penalty" coefficient related to the variability of each unit (method of the coefficient of variation penalty).

```{r, message=FALSE, warning=FALSE}
CI_MPI_estimated <- Compind::ci_mpi(this.indic.matrix.norm,
                           indic_col = (1:ncol(this.indic.matrix.norm)),
                           penalty = "NEG")  # Penalty direction; ”POS” (default) in case of increasing
#  or “positive” composite index (e.g., well-being index),
#  ”NEG” in case of decreasing or “negative” composite
#  index (e.g., poverty index).

ci_mpi_est <- data.frame( CI_MPI_estimated$ci_mpi_est)
names(ci_mpi_est) <- "Mazziotta_Pareto"
```


### Wroclaw taxonomy method 
 
This last method  (also known as the dendric method), originally developed at the University of Wroclaw, is based on the distance from a theoretical unit characterized by the best performance for all indicators considered; the composite indicator is therefore based on the sum of euclidean distances from the ideal unit and normalized by a measure of variability of these distance (mean + 2*std).

```{r, message=FALSE, warning=FALSE}
CI_wroclaw_estimated <-  Compind::ci_wroclaw(this.indic.matrix.norm,
                                    indic_col = (1:ncol(this.indic.matrix.norm)))

ci_wroclaw_est <- data.frame( CI_wroclaw_estimated$ci_wroclaw_est)
names(ci_wroclaw_est) <- "Wroclaw"

```

## Visualise output

### In a table

```{r, message=FALSE, warning=FALSE}
this.indic.matrix.norm2 <- cbind( #row.names(scores.this),
  ci_bod_est, # Benefit of the Doubt approach
  ci_rbod_est, # Robust Benefit of the Doubt approach
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  #ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  ci_factor_est, # Factor analysis  componnents
  ci_mean_geom_est, # Geometric aggregation
  ci_mean_min_est, # Mean-Min Function
  ci_mpi_est, # Mazziotta-Pareto Index
  ci_wroclaw_est) # Wroclaw taxonomy method

# 
# kable(this.indic.matrix.norm2, caption = "Composite with different algorithm") %>%
#            kableExtra::kable_styling(bootstrap_options = c("striped", "bordered", "condensed", "responsive"), font_size = 9)

```

As we can see some of the potential aggregation algorithm are not providing results from some location. We will therefore exclude them from the rest of the analysis.

```{r, message=FALSE, warning=FALSE}
## Eliminate automatically method that could not score some elements
this.indic.matrix.norm22 <- this.indic.matrix.norm2[, colSums(this.indic.matrix.norm2 != 0, na.rm = TRUE) > 0]

## Check if sum is  zero
this.indic.matrix.norm22 <- this.indic.matrix.norm22[, colSums(this.indic.matrix.norm22 != 0, na.rm = TRUE)  == nrow(this.indic.matrix.norm22)]

## Remove indic if standard deviation is o
this.indic.matrix.norm22 <- this.indic.matrix.norm22[, sapply(this.indic.matrix.norm22, function(x) { sd(x) != 0} )]

## Remove “NaN” or “Not a Number
this.indic.matrix.norm22 %>%
  summarise_all(function(x) sum(x[!is.na(x)] == "NaN") == length(x[!is.na(x)])) %>% # check if number of NaN is equal to number of rows after removing NAs
  select_if(function(x) x == FALSE) %>%       # select columns that don't have only nulls
  names() -> vars_to_keep                     # keep column names

# select columns captured above
this.indic.matrix.norm22 <- this.indic.matrix.norm22[ , vars_to_keep]                 



rm( ci_bod_constr_est_mpi, # Robust Benefit of the Doubt approach with constraint
  CI_BoD_MPI_estimated,
  ci_bod_est, # Benefit of the Doubt approach
  CI_BoD_estimated,
  ci_bod_dir_est, # Directional Robust Benefit of the Doubt approach
  CI_Direct_BoD_estimated,
  ci_rbod_est, # Robust Benefit of the Doubt approach
  CI_RBoD_estimated,
  ci_factor_est, # Factor analysis  componnents,
  dimfactor,
  CI_Factor_estimated,
  ci_mean_min_est, # Mean-Min Function
  CI_mean_min_estimated,
  ci_mpi_est, # Mazziotta-Pareto Index
  CI_MPI_estimated,
  ci_mean_geom_est, # Geometric aggregation
  CI_Geom_estimated,
  ci_wroclaw_est,# Wroclaw taxonomy method
  CI_wroclaw_estimated) 


```

###  Differences between algorithms

The various Index can be normalised again on a 0 to 1 scale in order to be compared. A specific treatment if necessary for index based on Factor analysis

```{r, message=FALSE, warning=FALSE, fig.height= 10, fig.width= 10}

## Directory of algorythms..
methodo <- c("Benef_Doubt",
          "Benef_Doubt_Rob",
          "Benef_Doubt_Dir",
          "Benef_Doubt_Cons",
          "Factor",
          "Mean_Geom",
          "Mean_Min",
          "Mazziotta_Pareto",
          "Wroclaw")

label <- c(    "Benefit of the Doubt Approach",
               "Robust Benefit of the Doubt Approach",
               "Directional Robust Benefit of the Doubt Approach",
               "Robust Benefit of the Doubt approach with constraint",
               "Factor Analysis Componnents",
               "Geometric Aggregation",
               "Mean-Min Function",
               "Mazziotta-Pareto Index",
               "Wroclaw Taxonomy")
polarity <- c( "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "POS",
                "NEG")

All.method <- as.data.frame( cbind(methodo, label,polarity))


## Factor analysis can provide negative rank - If it works we need to get rid of them
for (j in 1:nrow(this.indic.matrix.norm22)) {
this.indic.matrix.norm22[j ,c("Factor")] <- ifelse( min(this.indic.matrix.norm22[ ,c("Factor")]) < 0 ,
                                                    this.indic.matrix.norm22[j ,c("Factor")] + 
                                                      abs(min(this.indic.matrix.norm22[ ,c("Factor")])) ,
                                                     this.indic.matrix.norm22[j ,c("Factor")]) 
}

kept.methodo <- as.data.frame(names(this.indic.matrix.norm22))
polarity2 <- as.character(All.method[ All.method$methodo %in% names(this.indic.matrix.norm22),
                                      c("polarity") ])


this.indic.matrix.norm3 <- Compind::normalise_ci(this.indic.matrix.norm22,
                                        c(1:ncol(this.indic.matrix.norm22)),
                                        polarity =  polarity2,
                                        method = 3)
this.indic.matrix.norm3 <- this.indic.matrix.norm3$ci_norm

# kable(this.indic.matrix.norm3, caption = "Location Ranking with different algorithms") %>%
#            kableExtra::kable_styling(bootstrap_options = c("striped", "bordered", "condensed", "responsive"), font_size = 9)



```

Let's write this back to the excel doc

```{r, message=FALSE, warning=FALSE, fig.height= 10, fig.width= 10}
datawrite <- cbind(this.indic.df,
  this.indic.matrix.norm,
  this.indic.matrix.norm22,
  this.indic.matrix.norm3 )
# wb <- loadWorkbook("Areas.xlsx")
# choicesSheet <- xlsx::createSheet(wb, sheetName = this.dimension)
# xlsx::addDataFrame(datawrite, choicesSheet, col.names = TRUE, row.names = TRUE)
# xlsx::saveWorkbook(wb, "Areas.xlsx")
```

We can now build a visualization for the comparison between different valid methods.

```{r, message=FALSE, warning=FALSE, fig.height= 10, fig.width= 10}
## Remove NaN
this.indic.matrix.norm4 <-  this.indic.matrix.norm3[,colSums(this.indic.matrix.norm3 != 0, na.rm = TRUE) > 0]

## keep that frame for later on for the viz
assign(  paste("scores.", this.dimension, sep = ""), this.indic.matrix.norm3 )

## Add blank variable for nice chart display
this.indic.matrix.norm4$Location <- NA
  
this.indic.matrix.norm4.melt <- reshape::melt(as.matrix(this.indic.matrix.norm4))


#Make plot
ggplot(this.indic.matrix.norm4.melt, aes(x = X2,
                                                 y = value,
                                                 color = X1,
                                                 group = X1)) +
  geom_line(size = 2) +
  # scale_colour_manual(values = c("#8dd3c7","#A6CEE3", "#1F78B4", "#B2DF8A", "#33A02C",
  #                                "#FB9A99", "#E31A1C", "#FDBF6F", "#FF7F00", "#CAB2D6",
  #                                "#6A3D9A", "#fb8072", "#B15928", "#fdb462","#ccebc5",
  #                                "#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442",
  #                                "#0072B2", "#D55E00", "#CC79A7")) + ## as many color as locations.. 23
  ggrepel::geom_text_repel(
    data = this.indic.matrix.norm4.melt[ this.indic.matrix.norm4.melt$Var2 == "Wroclaw", ],
    aes(label = X1),
    arrow = arrow(length = unit(0.03, "npc"), type = "closed", ends = "first"),
    direction =	"y",
    size = 4,
    nudge_x = 45 ) +
  labs(title = paste0("Rank for Composite Indicator on ",  this.dimension ),
       subtitle = "Based on various weighting approach") +
  theme( plot.title = element_text(size = 13),
         plot.subtitle = element_text(size = 11),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 10),
         strip.text.x = element_text(size = 11),
         panel.grid.major.x = element_line(color = "#cbcbcb"),
         panel.grid.major.y = element_blank(),
         axis.text.x = element_text(angle = 45, hjust = 1),
         legend.position = "none")



```


### Index sensitivity to method 

As we can see, the final ranking for each location is very sensitive to the methods.

An approach to select the method can be to identify, average ranks per method in order to identify the method that is getting closer this average ranks.

we can first visualise those average ranks.

```{r, message=FALSE, warning=FALSE, echo = FALSE}


## Removing blank variable for nice chart display
this.indic.matrix.norm4$Location <- NULL

this.indic.matrix.norm5 <- this.indic.matrix.norm4
this.indic.matrix.norm5$Average <- rowMeans(this.indic.matrix.norm4, na.rm = FALSE, dims = 1:ncol(this.indic.matrix.norm4))

this.indic.matrix.norm5$Min <- matrixStats::rowMins(as.matrix(this.indic.matrix.norm4))
this.indic.matrix.norm5$Max <- matrixStats::rowMaxs(as.matrix(this.indic.matrix.norm4)) 
this.indic.matrix.norm5$Location <- row.names(this.indic.matrix.norm4)

this.indic.matrix.norm5$Location <- reorder(this.indic.matrix.norm5$Location, this.indic.matrix.norm5$Average)

plot1 <- ggplot(data = this.indic.matrix.norm5, 
                aes( x = Location, 
                     y = Average, 
                     ymin = Min,
                     ymax = Max)) +
  # geom_pointrange(color = "purple", shape = 18,  size = 2) + 
  
  geom_point(color = "purple", shape = 18,  size = 5) + 
  geom_errorbar(aes(xmax = Max, xmin = Min), height = 1, color = "black", width = 0.2, size = 1) +
  
  #geom_hline(yintercept = 1, lty = 2) +  # add a dotted line at x=1 after flip
  coord_flip() +  # flip coordinates (puts labels on y axis)
  xlab("Preference Level") +
  ylab("") +
  labs(title = paste0("Sensitivity Visualisation for ",
                      this.dimension ) ,
       subtitle =  "Average, Max and Min ranks based on aggregation methods",
       caption = "Dots represent the average. Black lines around the point represent the the maximum and minum ranks depending on methods") +
   
  theme( plot.title = element_text(size = 15),
         plot.subtitle = element_text(size = 12),
         plot.caption = element_text(size = 7, hjust = 1),
         axis.text = element_text(size = 9),
         panel.grid.major.x = element_line(color = "#cbcbcb"), 
         panel.grid.major.y = element_blank(),
         strip.text.x = element_text(size = 11))

ggpubr::ggarrange(left_align(plot1, c("subtitle", "title", "caption")), ncol = 1, nrow = 1)

```

Next is to compute standard deviation for each method.


```{r, message=FALSE, echo = FALSE, warning=FALSE}


### Identify less sensitve method
sensitivity <- data.frame(
  method  = character(),
  spread  = numeric(), 
  stringsAsFactors = FALSE
)

for (i in 1:ncol(this.indic.matrix.norm4)) {
  sensitivity[ i, c("method")]   <- names(this.indic.matrix.norm4)[i]
  sensitivity[ i,  c("spread")]  <- sd(this.indic.matrix.norm5[ , c("Average")] - this.indic.matrix.norm5[ , i ] )
  }

this.var  <- as.character(sensitivity[ which.min(sensitivity$spread), c("method")])
this.label <- as.character(All.method[All.method$methodo == this.var , c("label")])
this.composite <- as.data.frame(this.indic.matrix.norm22[ ,this.var ])
names(this.composite)[1] <- this.var
polyLevel2_sf <- cbind(polyLevel2_sf, this.composite)

#quint <- quantile(this.composite[ ,1], na.rm = TRUE)

#table(Hmisc::cut2(this.composite[ ,1], g = 6) )

polyLevel2_sf$quint2 <- Hmisc::cut2(this.composite[ ,1], g = 6) 
save(polyLevel2_sf,file= paste0(mainDirroot,"/data-raw/polyLevel2_sf.Rda"))
```

### Severity Index on a map

We can now visualize the thematic indicator on a map.

An initial visualization will be to present both the severity and the population size affected by this severity.

```{r, message=FALSE, warning=FALSE, fig.height= 10, fig.width= 10}

## Color scale - https://developer.r-project.org/Blog/public/2019/04/01/hcl-based-color-palettes-in-grdevices/

mapsf::mf_init(polyLevel2_sf) 
mapsf::mf_map(polyLevel2_sf, 
              add = TRUE, 
              lwd = 0.5, 
              border = "#93A3AB", 
              col = "#FFFFFF")
mapsf::mf_map(ocean, 
              col = "#B3D8F0", 
              border = NA, 
              add = TRUE)

mapsf::mf_prop_typo( 
  x = polyLevel2_sf, 
  var = c("total_pop", "quint2"),
  inches = .35, 
  border = "tomato4",
  alpha = .8,
  val_max = 90000, 
  symbol = "circle", 
  col_na = "grey", 
  pal = "Inferno",
  lwd = 1,
  leg_pos = c("bottomright", "bottomleft"),
  leg_title = c("Population", "Severity Index"),
  leg_title_cex = c(0.9, 0.9),
  leg_val_cex = c(.7, .7),
  #val_order = c("Prefecture", "Sub-prefecture", "Simple municipality"),
  leg_no_data = "No dada",
  leg_frame = c(TRUE, TRUE),
  add = TRUE
)

# Set a layout
mapsf::mf_title(txt = "Areas Characterisation - El Salvador", 
                fg = "#FFFFFF")
mapsf::mf_credits(txt = "Source: Multiple Indicators aggregated at level 2",
                   bg = "#ffffff80") 


```


 
# Annex

## Leveraging Facebook data for Good dataset


People who use Facebook on a mobile device have the option of providing their precise location in order to enable products like Nearby Friends and Find Wi-Fi and to get local content and ads. Different type of products are produced by the [Facebok Data for Good Team]() by aggregating and de-identifying this data. Only people who opt in to Location History and background location collection are included. People with very few location pings in a day are not informative for these trends, and, therefore, we include only those people whose location is observed for a meaningful period of the day.

Central America indeed has an important [number of facebook users]and Facebook data for good has released a few [dataset](https://data.humdata.org/organization/facebook?groups=slv)


## How to re-use that script

Get the correct project name in HDX

